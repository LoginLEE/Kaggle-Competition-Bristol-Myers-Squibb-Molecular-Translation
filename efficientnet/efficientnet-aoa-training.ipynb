{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "\n",
    "This notebook builds on the great pipeline [introduced](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter) by Y. Nakama and [adapted to EfficientNets](https://www.kaggle.com/konradb/model-train-efficientnet) by Konrad Banachewicz. The notebook further extendens the pipeline by adding support for multi-layer LSTM in the decoder part. Most of the code changes are concentrated in the model class. Please credit the original authors for their contributions.\n",
    "\n",
    "This is training notebook. Inference with multi-layer LSTM decoder is demonstarted [in this notebook](https://www.kaggle.com/kozodoi/efficientnet-multi-layer-lstm-inference).\n",
    "\n",
    "### References:\n",
    "\n",
    "- [starter notebook from Y. Nakama](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n",
    "- [adapted notebook from Konrad](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n",
    "- [PyTorch tutorial on image captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)\n",
    "- [two-layer RNN implementation](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/pull/79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.105933,
     "end_time": "2021-04-07T08:27:02.380708",
     "exception": false,
     "start_time": "2021-04-07T08:26:57.274775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "import albumentations as A\n",
    "import timm\n",
    "\n",
    "from aoa import AoA_Refiner_Core\n",
    "\n",
    "# from axial_attention import AxialAttention\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFG class now includes a new parameter: `decoder_layers`. For illustration purposes, I am running a two-layer LSTM for 1 epoch on 100k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022485,
     "end_time": "2021-04-07T08:27:10.713561",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.691076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n",
    "#   'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n",
    "#   'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n",
    "\n",
    "# This is not, to put it mildly, the most elegant solution ever - but I ran into some trouble \n",
    "# with checking the size of feature spaces programmatically inside the CFG definition.\n",
    "\n",
    "class CFG:\n",
    "    debug          = True\n",
    "    apex           = False\n",
    "    max_len        = 275\n",
    "    print_freq     = 250\n",
    "    num_workers    = 16\n",
    "    model_name     = 'efficientnet_b2'\n",
    "    enc_size       = 1408\n",
    "    samp_size      = 100000\n",
    "    size           = 288\n",
    "    scheduler      = 'CosineAnnealingLR' \n",
    "    epochs         = 6\n",
    "    T_max          = 6  \n",
    "    encoder_lr     = 1e-4\n",
    "    decoder_lr     = 4e-4\n",
    "    min_lr         = 1e-6\n",
    "    batch_size     = 64\n",
    "    weight_decay   = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm  = 10\n",
    "    attention_dim  = 256\n",
    "    embed_dim      = 512\n",
    "    decoder_dim    = 512\n",
    "    decoder_layers = 2     # number of LSTM layers\n",
    "    dropout        = 0.5\n",
    "    seed           = 42\n",
    "    n_fold         = 5\n",
    "#     trn_fold       = 1\n",
    "    trn_fold       = 3\n",
    "    train          = True\n",
    "    train_path     = '../data/train/'\n",
    "    test_path     = '../data/test/'\n",
    "    prep_path      = './'\n",
    "#     prev_model     = './best.pth'\n",
    "    prev_model = \"./efficientnet_b2_fold0_best.pth\"\n",
    "    pred_model = \"./efficientnet_b2_fold0_best.pth\"\n",
    "    rnn_size = 1408\n",
    "    multi_head_scale = 1\n",
    "    num_heads = 8\n",
    "    vocab_size = 193\n",
    "    input_encoding_size = 512\n",
    "    num_layers = 2\n",
    "    drop_prob_lm = 0.5\n",
    "    att_feat_size = 512\n",
    "    use_ff = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011683,
     "end_time": "2021-04-07T08:27:10.737552",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.725869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.037989,
     "end_time": "2021-04-07T08:27:10.787242",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.749253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "tokenizer = torch.load(CFG.prep_path + 'tokenizer.pth')\n",
    "print(f\"tokenizer.stoi: {tokenizer.stoi}\")\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027729,
     "end_time": "2021-04-07T08:27:10.827442",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.799713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        score = Levenshtein.distance(true, pred)\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed = CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024936,
     "end_time": "2021-04-07T08:27:10.869493",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.844557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None):\n",
    "        super().__init__()\n",
    "        self.df         = df\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.labels     = df['InChI_text'].values\n",
    "        self.transform  = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image = image)\n",
    "            image     = augmented['image']\n",
    "        label = self.labels[idx]\n",
    "        label = self.tokenizer.text_to_sequence(label)\n",
    "        label_length = len(label)\n",
    "        label_length = torch.LongTensor([label_length])\n",
    "        return image, torch.LongTensor(label), label_length\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        super().__init__()\n",
    "        self.df            = df\n",
    "        self.file_paths    = df['file_path'].values\n",
    "        self.transform     = transform\n",
    "        self.fix_transform = A.Compose([A.Transpose(p=1), A.VerticalFlip(p=1)])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        h, w, _ = image.shape\n",
    "        if h > w:\n",
    "            image = self.fix_transform(image=image)['image']\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image     = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021152,
     "end_time": "2021-04-07T08:27:10.902922",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.88177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bms_collate(batch):\n",
    "    imgs, labels, label_lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        imgs.append(data_point[0])\n",
    "        labels.append(data_point[1])\n",
    "        label_lengths.append(data_point[2])\n",
    "    labels = pad_sequence(labels, batch_first = True, padding_value = tokenizer.stoi[\"<pad>\"])\n",
    "    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021209,
     "end_time": "2021-04-07T08:27:10.936685",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.915476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### CNN ENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name = CFG.model_name, refiner_opt = CFG, pretrained = False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n",
    "        self.refiner = AoA_Refiner_Core(refiner_opt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs       = x.size(0)\n",
    "        features = self.cnn.forward_features(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        features = features.view(features.shape[0],-1, features.shape[-1])\n",
    "        features = self.refiner(features, None)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `DecoderWithAttention` is updated to support a multi-layer LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.067717,
     "end_time": "2021-04-07T08:27:11.017304",
     "exception": false,
     "start_time": "2021-04-07T08:27:10.949587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### RNN DECODER\n",
    "\n",
    "# attention module\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention network for calculate attention value\n",
    "    '''\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        '''\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        '''\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n",
    "        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "    \n",
    "# custom LSTM cell\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# decoder\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    '''\n",
    "    Decoder network with attention network used for training\n",
    "    '''\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n",
    "        '''\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param num_layers: number of the LSTM layers\n",
    "        :param dropout: dropout rate\n",
    "        '''\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim   = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim     = embed_dim\n",
    "        self.decoder_dim   = decoder_dim\n",
    "        self.vocab_size    = vocab_size\n",
    "        self.dropout       = dropout\n",
    "        self.num_layers    = num_layers\n",
    "        self.device        = device\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n",
    "        self.dropout       = nn.Dropout(p = self.dropout)\n",
    "        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n",
    "        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid       = nn.Sigmoid()\n",
    "        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()                                      # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune = True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim = 1)\n",
    "        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n",
    "        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        '''\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        '''\n",
    "        batch_size       = encoder_out.size(0)\n",
    "        encoder_dim      = encoder_out.size(-1)\n",
    "        vocab_size       = self.vocab_size\n",
    "        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels       = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        \n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        \n",
    "        # Initialize LSTM state, initialize cell_vector and hidden_vector\n",
    "        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n",
    "#         alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, _ = self.attention(encoder_out[:batch_size_t], prev_h[-1][:batch_size_t])\n",
    "\n",
    "            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
    "            \n",
    "            for i, rnn in enumerate(self.decode_step):\n",
    "                # recurrent cell\n",
    "                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n",
    "\n",
    "                # hidden state becomes the input to the next layer\n",
    "                input = self.dropout(h)\n",
    "\n",
    "                # save state for next time step\n",
    "                prev_h[i] = h\n",
    "                prev_c[i] = c\n",
    "                \n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "#             alphas[:batch_size_t, t, :]      = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        \n",
    "        # size variables\n",
    "        batch_size  = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size  = self.vocab_size\n",
    "#         encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels  = encoder_out.size(1)\n",
    "        \n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n",
    "        embeddings    = self.embedding(start_tockens)\n",
    "        \n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n",
    "        for t in range(decode_lengths):\n",
    "            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "            awe        = gate * awe\n",
    "            \n",
    "            input = torch.cat([embeddings, awe], dim=1)\n",
    " \n",
    "            for j, rnn in enumerate(self.decode_step):\n",
    "                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "                input = self.dropout(at_h)\n",
    "                h[j]  = at_h\n",
    "                c[j]  = at_c\n",
    "            \n",
    "            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n",
    "            if end_condition.sum() == batch_size:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    # beam search\n",
    "    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n",
    "        \n",
    "        h, c = hidden\n",
    "        #h, c = h.squeeze(0), c.squeeze(0)\n",
    "        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n",
    "        \n",
    "        embeddings = self.embedding(prev_tokens)\n",
    "        if embeddings.dim() == 3:\n",
    "            embeddings = embeddings.squeeze(1)\n",
    "            \n",
    "        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "        awe        = gate * awe\n",
    "        \n",
    "        input = torch.cat([embeddings, awe], dim = 1)\n",
    "        for j, rnn in enumerate(self.decode_step):\n",
    "            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "            input = self.dropout(at_h)\n",
    "            h[j]  = at_h\n",
    "            c[j]  = at_c\n",
    "\n",
    "        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "\n",
    "        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n",
    "        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n",
    "        predicted_softmax = function(preds, dim = 1)\n",
    "        \n",
    "        return predicted_softmax, hidden, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.039205,
     "end_time": "2021-04-07T08:27:11.070219",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.031014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val   = 0\n",
    "        self.avg   = 0\n",
    "        self.sum   = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val    = val\n",
    "        self.sum   += val * n\n",
    "        self.count += n\n",
    "        self.avg    = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s   = now - since\n",
    "    es  = s / (percent)\n",
    "    rs  = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, encoder, decoder, criterion, \n",
    "             encoder_optimizer, decoder_optimizer, epoch,\n",
    "             encoder_scheduler, decoder_scheduler, device):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    losses     = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    for step, (images, labels, label_lengths) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images        = images.to(device)\n",
    "        labels        = labels.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        batch_size    = images.size(0)\n",
    "        \n",
    "        features = encoder(images)\n",
    "        predictions, caps_sorted, decode_lengths, sort_ind = decoder(features, labels, label_lengths)\n",
    "        targets     = caps_sorted[:, 1:]\n",
    "        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n",
    "        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        loss        = criterion(predictions, targets)\n",
    "        \n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "            \n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n",
    "        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n",
    "                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n",
    "                  #'Encoder LR: {encoder_lr:.6f}  '\n",
    "                  #'Decoder LR: {decoder_lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), \n",
    "                   batch_time        = batch_time,\n",
    "                   data_time         = data_time, \n",
    "                   loss              = losses,\n",
    "                   remain            = timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   encoder_grad_norm = encoder_grad_norm,\n",
    "                   decoder_grad_norm = decoder_grad_norm,\n",
    "                   #encoder_lr=encoder_scheduler.get_lr()[0],\n",
    "                   #decoder_lr=decoder_scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    text_preds = []\n",
    "    start = end = time.time()\n",
    "    \n",
    "    for step, (images) in enumerate(valid_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images     = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features    = encoder(images)\n",
    "            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n",
    "            \n",
    "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "        _text_preds        = tokenizer.predict_captions(predicted_sequence)\n",
    "        text_preds.append(_text_preds)\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), \n",
    "                   batch_time = batch_time,\n",
    "                   data_time  = data_time,\n",
    "                   remain     = timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "            \n",
    "    text_preds = np.concatenate(text_preds)\n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.038367,
     "end_time": "2021-04-07T08:27:11.123364",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.084997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds  = folds.loc[trn_idx].reset_index(drop = True)\n",
    "    valid_folds  = folds.loc[val_idx].reset_index(drop = True)\n",
    "    valid_labels = valid_folds['InChI'].values\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, tokenizer, transform = get_transforms(data = 'train'))\n",
    "    valid_dataset = TestDataset(valid_folds, transform = get_transforms(data = 'valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size  = CFG.batch_size, \n",
    "                              shuffle     = True, \n",
    "                              num_workers = CFG.num_workers, \n",
    "                              pin_memory  = True,\n",
    "                              drop_last   = True, \n",
    "                              collate_fn  = bms_collate)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size  = CFG.batch_size, \n",
    "                              shuffle     = False, \n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory  = True, \n",
    "                              drop_last   = False)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                          mode     = 'min', \n",
    "                                          factor   = CFG.factor, \n",
    "                                          patience = CFG.patience, \n",
    "                                          verbose  = True, \n",
    "                                          eps      = CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, \n",
    "                                          T_max      = CFG.T_max, \n",
    "                                          eta_min    = CFG.min_lr, \n",
    "                                          last_epoch = -1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                    T_0        = CFG.T_0, \n",
    "                                                    T_mult     = 1, \n",
    "                                                    eta_min    = CFG.min_lr, \n",
    "                                                    last_epoch = -1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    states = torch.load(CFG.prev_model,  map_location=device)\n",
    "\n",
    "    encoder = Encoder(CFG.model_name, CFG,\n",
    "                      pretrained = True)\n",
    "    encoder.load_state_dict(states['encoder'])\n",
    "    \n",
    "    encoder.to(device)\n",
    "    encoder_optimizer = Adam(encoder.parameters(), \n",
    "                             lr           = CFG.encoder_lr, \n",
    "                             weight_decay = CFG.weight_decay, \n",
    "                             amsgrad      = False)\n",
    "    encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n",
    "    encoder_scheduler = get_scheduler(encoder_optimizer)\n",
    "    encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n",
    "                                   embed_dim     = CFG.embed_dim, \n",
    "                                   encoder_dim   = CFG.enc_size,\n",
    "                                   decoder_dim   = CFG.decoder_dim,\n",
    "                                   num_layers    = CFG.decoder_layers,\n",
    "                                   vocab_size    = len(tokenizer), \n",
    "                                   dropout       = CFG.dropout, \n",
    "                                   device        = device)\n",
    "    decoder.load_state_dict(states['decoder'])\n",
    "    decoder.to(device)\n",
    "    decoder_optimizer = Adam(decoder.parameters(), \n",
    "                             lr           = CFG.decoder_lr, \n",
    "                             weight_decay = CFG.weight_decay, \n",
    "                             amsgrad      = False)\n",
    "    decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n",
    "\n",
    "    decoder_scheduler = get_scheduler(decoder_optimizer)\n",
    "    decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_loss  = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n",
    "                            encoder_optimizer, decoder_optimizer, epoch, \n",
    "                            encoder_scheduler, decoder_scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n",
    "        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n",
    "        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n",
    "        LOGGER.info(f\"preds: {text_preds[:5]}\")\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, text_preds)\n",
    "        \n",
    "        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n",
    "            encoder_scheduler.step(score)\n",
    "        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n",
    "            encoder_scheduler.step()\n",
    "        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n",
    "            encoder_scheduler.step()\n",
    "            \n",
    "        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n",
    "            decoder_scheduler.step(score)\n",
    "        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n",
    "            decoder_scheduler.step()\n",
    "        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n",
    "            decoder_scheduler.step()\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'encoder': encoder.state_dict(), \n",
    "                        'encoder_optimizer': encoder_optimizer.state_dict(), \n",
    "                        'encoder_scheduler': encoder_scheduler.state_dict(), \n",
    "                        'decoder': decoder.state_dict(), \n",
    "                        'decoder_optimizer': decoder_optimizer.state_dict(), \n",
    "                        'decoder_scheduler': decoder_scheduler.state_dict(), \n",
    "                        'text_preds': text_preds,\n",
    "                       },\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020522,
     "end_time": "2021-04-07T08:27:11.156788",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.136266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_file_path(image_id):\n",
    "\n",
    "    return CFG.train_path + \"{}/{}/{}/{}.png\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )\n",
    "def get_test_file_path(image_id):\n",
    "\n",
    "    return CFG.test_path + \"{}/{}/{}/{}.png\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022257,
     "end_time": "2021-04-07T08:27:11.192091",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.169834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformations\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            HorizontalFlip(p=0.5),                  \n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),   \n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013404,
     "end_time": "2021-04-07T08:27:11.218463",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.205059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.663809,
     "end_time": "2021-04-07T08:27:23.895404",
     "exception": false,
     "start_time": "2021-04-07T08:27:11.231595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (2424186, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(CFG.prep_path + 'train.pkl')\n",
    "\n",
    "train['file_path'] = train['image_id'].apply(get_train_file_path)\n",
    "\n",
    "print(f'train.shape: {train.shape}')\n",
    "\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n = CFG.samp_size, random_state = CFG.seed).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02079,
     "end_time": "2021-04-07T08:27:23.931533",
     "exception": false,
     "start_time": "2021-04-07T08:27:23.910743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(train, tokenizer, transform = get_transforms(data='train'))\n",
    "\n",
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013921,
     "end_time": "2021-04-07T08:27:24.004031",
     "exception": false,
     "start_time": "2021-04-07T08:27:23.99011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7272] Data 0.272 (0.272) Elapsed 0m 0s (remain 106m 29s) Loss: 0.4232(0.4232) Encoder Grad: 0.5813  Decoder Grad: 0.5105  \n",
      "Epoch: [1][250/7272] Data 0.000 (0.001) Elapsed 2m 38s (remain 74m 6s) Loss: 0.4530(0.4818) Encoder Grad: 0.6567  Decoder Grad: 0.6602  \n",
      "Epoch: [1][500/7272] Data 0.000 (0.001) Elapsed 5m 16s (remain 71m 21s) Loss: 0.5005(0.4795) Encoder Grad: 0.7052  Decoder Grad: 0.4476  \n",
      "Epoch: [1][750/7272] Data 0.000 (0.001) Elapsed 7m 54s (remain 68m 41s) Loss: 0.4229(0.4822) Encoder Grad: 0.7231  Decoder Grad: 0.4527  \n",
      "Epoch: [1][1000/7272] Data 0.000 (0.001) Elapsed 10m 32s (remain 65m 59s) Loss: 0.4260(0.4808) Encoder Grad: 0.5695  Decoder Grad: 0.3945  \n",
      "Epoch: [1][1250/7272] Data 0.000 (0.000) Elapsed 13m 9s (remain 63m 20s) Loss: 0.4173(0.4817) Encoder Grad: 0.5589  Decoder Grad: 0.5581  \n",
      "Epoch: [1][1500/7272] Data 0.000 (0.000) Elapsed 15m 47s (remain 60m 42s) Loss: 0.6310(0.4819) Encoder Grad: 0.8606  Decoder Grad: 0.6478  \n",
      "Epoch: [1][1750/7272] Data 0.000 (0.000) Elapsed 18m 25s (remain 58m 4s) Loss: 0.3721(0.4827) Encoder Grad: 0.6639  Decoder Grad: 0.6548  \n",
      "Epoch: [1][2000/7272] Data 0.000 (0.000) Elapsed 21m 2s (remain 55m 26s) Loss: 0.3566(0.4836) Encoder Grad: 0.6068  Decoder Grad: 0.5202  \n",
      "Epoch: [1][2250/7272] Data 0.000 (0.000) Elapsed 23m 42s (remain 52m 53s) Loss: 0.5241(0.4838) Encoder Grad: 1.0566  Decoder Grad: 0.8890  \n",
      "Epoch: [1][2500/7272] Data 0.000 (0.000) Elapsed 26m 21s (remain 50m 16s) Loss: 0.5779(0.4834) Encoder Grad: 0.6444  Decoder Grad: 0.5993  \n",
      "Epoch: [1][2750/7272] Data 0.000 (0.000) Elapsed 29m 0s (remain 47m 41s) Loss: 0.4605(0.4830) Encoder Grad: 0.5299  Decoder Grad: 0.5231  \n",
      "Epoch: [1][3000/7272] Data 0.000 (0.000) Elapsed 31m 45s (remain 45m 11s) Loss: 0.5181(0.4824) Encoder Grad: 0.6671  Decoder Grad: 0.9496  \n",
      "Epoch: [1][3250/7272] Data 0.000 (0.000) Elapsed 34m 29s (remain 42m 39s) Loss: 0.4154(0.4819) Encoder Grad: 0.6620  Decoder Grad: 0.4685  \n",
      "Epoch: [1][3500/7272] Data 0.000 (0.000) Elapsed 37m 13s (remain 40m 5s) Loss: 0.3822(0.4814) Encoder Grad: 0.7374  Decoder Grad: 0.4387  \n",
      "Epoch: [1][3750/7272] Data 0.000 (0.000) Elapsed 39m 58s (remain 37m 31s) Loss: 0.5894(0.4813) Encoder Grad: 0.8375  Decoder Grad: 0.4753  \n",
      "Epoch: [1][4000/7272] Data 0.000 (0.000) Elapsed 42m 42s (remain 34m 55s) Loss: 0.4380(0.4811) Encoder Grad: 0.7498  Decoder Grad: 0.5519  \n",
      "Epoch: [1][4250/7272] Data 0.000 (0.000) Elapsed 45m 25s (remain 32m 17s) Loss: 0.3279(0.4810) Encoder Grad: 0.5381  Decoder Grad: 0.5556  \n",
      "Epoch: [1][4500/7272] Data 0.000 (0.000) Elapsed 48m 5s (remain 29m 36s) Loss: 0.4515(0.4810) Encoder Grad: 0.9691  Decoder Grad: 0.5509  \n",
      "Epoch: [1][4750/7272] Data 0.000 (0.000) Elapsed 50m 49s (remain 26m 57s) Loss: 0.4757(0.4805) Encoder Grad: 0.6508  Decoder Grad: 0.6538  \n",
      "Epoch: [1][5000/7272] Data 0.000 (0.000) Elapsed 53m 33s (remain 24m 19s) Loss: 0.4145(0.4800) Encoder Grad: 0.7060  Decoder Grad: 0.5238  \n",
      "Epoch: [1][5250/7272] Data 0.000 (0.000) Elapsed 56m 13s (remain 21m 38s) Loss: 0.5050(0.4795) Encoder Grad: 0.7426  Decoder Grad: 0.7073  \n",
      "Epoch: [1][5500/7272] Data 0.000 (0.000) Elapsed 58m 51s (remain 18m 56s) Loss: 0.5350(0.4790) Encoder Grad: 0.6601  Decoder Grad: 0.5115  \n",
      "Epoch: [1][5750/7272] Data 0.000 (0.000) Elapsed 61m 28s (remain 16m 15s) Loss: 0.5095(0.4785) Encoder Grad: 0.9163  Decoder Grad: 0.6520  \n",
      "Epoch: [1][6000/7272] Data 0.000 (0.000) Elapsed 64m 7s (remain 13m 34s) Loss: 0.3772(0.4780) Encoder Grad: 0.7732  Decoder Grad: 0.4668  \n",
      "Epoch: [1][6250/7272] Data 0.000 (0.000) Elapsed 66m 47s (remain 10m 54s) Loss: 0.4726(0.4776) Encoder Grad: 0.8732  Decoder Grad: 0.7410  \n",
      "Epoch: [1][6500/7272] Data 0.000 (0.000) Elapsed 69m 33s (remain 8m 14s) Loss: 0.6551(0.4768) Encoder Grad: 2.0650  Decoder Grad: 0.6340  \n",
      "Epoch: [1][6750/7272] Data 0.000 (0.000) Elapsed 72m 20s (remain 5m 34s) Loss: 0.4929(0.4766) Encoder Grad: 0.6763  Decoder Grad: 0.4466  \n",
      "Epoch: [1][7000/7272] Data 0.000 (0.000) Elapsed 75m 2s (remain 2m 54s) Loss: 0.5514(0.4761) Encoder Grad: 1.3785  Decoder Grad: 0.4968  \n",
      "Epoch: [1][7250/7272] Data 0.000 (0.000) Elapsed 77m 44s (remain 0m 13s) Loss: 0.5633(0.4754) Encoder Grad: 0.6881  Decoder Grad: 0.5657  \n",
      "Epoch: [1][7271/7272] Data 0.000 (0.000) Elapsed 77m 58s (remain 0m 0s) Loss: 0.5321(0.4754) Encoder Grad: 0.6182  Decoder Grad: 0.4534  \n",
      "EVAL: [0/1819] Data 0.254 (0.254) Elapsed 0m 0s (remain 12m 57s) \n",
      "EVAL: [250/1819] Data 0.000 (0.001) Elapsed 0m 46s (remain 4m 50s) \n",
      "EVAL: [500/1819] Data 0.000 (0.001) Elapsed 1m 32s (remain 4m 2s) \n",
      "EVAL: [750/1819] Data 0.000 (0.000) Elapsed 2m 18s (remain 3m 16s) \n",
      "EVAL: [1000/1819] Data 0.000 (0.000) Elapsed 3m 4s (remain 2m 31s) \n",
      "EVAL: [1250/1819] Data 0.000 (0.000) Elapsed 3m 51s (remain 1m 45s) \n",
      "EVAL: [1500/1819] Data 0.000 (0.000) Elapsed 4m 38s (remain 0m 59s) \n",
      "EVAL: [1750/1819] Data 0.000 (0.000) Elapsed 5m 25s (remain 0m 12s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "labels: ['InChI=1S/C21H22N6O4/c1-10-19-14(27-21(22)24-10)7-13(26-20(19)28)11-5-16(29-2)17(30-3)6-12(11)15-8-23-9-18(25-15)31-4/h5-6,8-9,13H,7H2,1-4H3,(H,26,28)(H2,22,24,27)/t13-/m1/s1'\n",
      " 'InChI=1S/C21H26N2O4S/c1-27-19-12-10-17(11-13-19)16-22-21(24)15-18-7-5-6-14-23(18)28(25,26)20-8-3-2-4-9-20/h2-4,8-13,18H,5-7,14-16H2,1H3,(H,22,24)/t18-/m1/s1'\n",
      " 'InChI=1S/C14H11BrN4O/c15-9-5-7-10(8-6-9)19-13(20)11-3-1-2-4-12(11)17-14(19)18-16/h1-8H,16H2,(H,17,18)'\n",
      " 'InChI=1S/C19H27N/c1-12-5-3-6-13(2)17(12)11-19(20)10-14-9-18(19)16-8-4-7-15(14)16/h3,5-6,14-16,18H,4,7-11,20H2,1-2H3'\n",
      " 'InChI=1S/C15H18ClN3O3S/c1-7-10-11(22-15(2,3)21-10)14(20-7)19-5-8(23-4)9-12(16)17-6-18-13(9)19/h5-7,10-11,14H,1-4H3/t7-,10-,11-,14-/m1/s1']\n",
      "preds: ['InChI=1S/C20H22N6O4/c1-10-16(21)23-16-8-15(24-20(27)24-13)9(2)16(24-11)11-6-16(28-3)18(29-4)13(7-11)21-10(5)25-21/h6-8,12H,9H2,1-5H3,(H,24,27)(H2,23,25,26,28)/t14-/m1/s1', 'InChI=1S/C21H26N2O4S/c1-26-19-12-10-17(11-13-18)15-22-21(24)15-18-8-5-6-14-23(18)28(25,26)20-9-4-2-3-7-19/h2-4,7-8,10-13,19H,5-6,9,14-16H2,1H3,(H,22,24)/t19-/m1/s1', 'InChI=1S/C14H11BrN4O/c15-9-5-7-11(8-6-9)18-13(19)10-3-1-2-4-12(10)17-14(19)18-16/h1-8H,16H2,(H,17,18)', 'InChI=1S/C19H27N/c1-12-4-3-5-13(2)17(12)11-18(20)10-15-9-15-6-7-16(16)17(14)8-15/h3-5,13-15,17H,6-11,19H2,1-2H3', 'InChI=1S/C15H16ClFN2O3S/c1-6-11-12(22-13(17-9)21-7)5-20-8-4-19(6-2)11(19)10(8)19-13-13(17)20-3/h4-7,10-11H,1-3H3/t6-,9+,11+,12+,14-/m1/s1']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1818/1819] Data 0.000 (0.000) Elapsed 5m 37s (remain 0m 0s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.4754  time: 5017s\n",
      "Epoch 1 - Score: 34.5247\n",
      "Epoch 1 - Save Best Score: 34.5247 Model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loop(folds, CFG.trn_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (1616107, 3)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "test['file_path'] = test['image_id'].apply(get_test_file_path)\n",
    "\n",
    "print(f'test.shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def inference(test_loader, encoder, decoder, tokenizer, device):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    text_preds = []\n",
    "    tk0 = tqdm(test_loader, total = len(test_loader))\n",
    "    \n",
    "    for images in tk0:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = encoder(images)\n",
    "            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n",
    "            \n",
    "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "        _text_preds = tokenizer.predict_captions(predicted_sequence)\n",
    "        text_preds.append(_text_preds)\n",
    "        \n",
    "    text_preds = np.concatenate(text_preds)\n",
    "    \n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21681"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# load model\n",
    "# ====================================================\n",
    "    \n",
    "states = torch.load(CFG.pred_model, map_location = torch.device('cpu'))\n",
    "\n",
    "encoder = Encoder(CFG.model_name, pretrained = False)\n",
    "encoder.load_state_dict(states['encoder'])\n",
    "encoder.to(device)\n",
    "\n",
    "decoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n",
    "                               embed_dim     = CFG.embed_dim, \n",
    "                               encoder_dim   = CFG.enc_size,\n",
    "                               decoder_dim   = CFG.decoder_dim,\n",
    "                               num_layers    = CFG.decoder_layers,\n",
    "                               vocab_size    = len(tokenizer), \n",
    "                               dropout       = CFG.dropout, \n",
    "                               device        = device)\n",
    "decoder.load_state_dict(states['decoder'])\n",
    "decoder.to(device)\n",
    "\n",
    "del states; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9549501db64042c0b07abd1ceec3e091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-13aef0625e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpredictions\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-efaf318cd3e1>\u001b[0m in \u001b[0;36minference\u001b[0;34m(test_loader, encoder, decoder, tokenizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpredicted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-06da0a3964de>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, encoder_out, decode_lengths, tokenizer)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mat_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (s, decoder_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mat_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[0]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         return _VF.lstm_cell(\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# inference\n",
    "# ====================================================\n",
    "\n",
    "test_dataset = TestDataset(test, transform = get_transforms(data = 'valid'))\n",
    "test_loader  = DataLoader(test_dataset, batch_size = CFG.batch_size, shuffle = False, num_workers = CFG.num_workers)\n",
    "predictions  = inference(test_loader, encoder, decoder, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "#  submission\n",
    "# ====================================================\n",
    "\n",
    "test['InChI'] = [f\"InChI=1S/{text}\" for text in predictions]\n",
    "test[['image_id', 'InChI']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "name": "efficientnet-multi-layer-lstm-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}