{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013887,
     "end_time": "2021-04-26T12:53:54.097098",
     "exception": false,
     "start_time": "2021-04-26T12:53:54.083211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SUMMARY\n",
    "\n",
    "This notebook builds on the great pipeline [introduced](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter) by Y. Nakama and [adapted to EfficientNets](https://www.kaggle.com/konradb/model-train-efficientnet) by Konrad Banachewicz. The notebook further extendens the pipeline by adding support for multi-layer LSTM in the decoder part. Most of the code changes are concentrated in the model class. Please credit the original authors for their contributions.\n",
    "\n",
    "This is training notebook. Inference with multi-layer LSTM decoder is demonstarted [in this notebook](https://www.kaggle.com/kozodoi/efficientnet-multi-layer-lstm-inference).\n",
    "\n",
    "### References:\n",
    "\n",
    "- [starter notebook from Y. Nakama](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n",
    "- [adapted notebook from Konrad](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n",
    "- [PyTorch tutorial on image captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)\n",
    "- [two-layer RNN implementation](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/pull/79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 4.802785,
     "end_time": "2021-04-26T12:53:58.912619",
     "exception": false,
     "start_time": "2021-04-26T12:53:54.109834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import Levenshtein\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "from aoa import AoA_Refiner_Core, clones\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013476,
     "end_time": "2021-04-26T12:53:58.939553",
     "exception": false,
     "start_time": "2021-04-26T12:53:58.926077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CFG class now includes a new parameter: `decoder_layers`. For illustration purposes, I am running a two-layer LSTM for 1 epoch on 100k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.022848,
     "end_time": "2021-04-26T12:53:58.975164",
     "exception": false,
     "start_time": "2021-04-26T12:53:58.952316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n",
    "#   'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n",
    "#   'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n",
    "\n",
    "# This is not, to put it mildly, the most elegant solution ever - but I ran into some trouble \n",
    "# with checking the size of feature spaces programmatically inside the CFG definition.\n",
    "\n",
    "class CFG:\n",
    "    debug          = False\n",
    "    apex           = False\n",
    "    max_len        = 275\n",
    "    print_freq     = 100\n",
    "    num_workers    = 16\n",
    "    model_name     = 'efficientnet_b2'\n",
    "    enc_size       = 1408\n",
    "    samp_size      = 1000\n",
    "    size           = 288\n",
    "    scheduler      = 'CosineAnnealingLR' \n",
    "    epochs         = 7\n",
    "    n_fold         = 20\n",
    "    batch_size     = 32+8\n",
    "    T_max          = int(((2424186 * (n_fold-1)/n_fold) / batch_size) * epochs )\n",
    "    encoder_lr     = 1e-4\n",
    "    decoder_lr     = 4e-4\n",
    "    min_lr         = 1e-7\n",
    "    weight_decay   = 1e-6\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm  = 10\n",
    "    attention_dim  = 256\n",
    "    embed_dim      = 512\n",
    "    decoder_dim    = 512\n",
    "    decoder_layers = 2     # number of LSTM layers\n",
    "    dropout        = 0.5\n",
    "    seed           = 32\n",
    "    trn_fold       = 0 \n",
    "    train          = True\n",
    "    train_path     = '../data/processed_train/'\n",
    "    test_path      = '../data/processed_test/'\n",
    "    prep_path      = './'\n",
    "    prev_models     = ['./efficientnet_b2_fold0_best_synthetic.pth', './efficientnet_b2_fold0_best.pth']\n",
    "    prev_model     = './efficientnet_b2_fold0_best_aoa_en.pth'\n",
    "    aoa_use_ff     = 0\n",
    "    num_heads = 8\n",
    "    ensemble_num = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012553,
     "end_time": "2021-04-26T12:53:59.000553",
     "exception": false,
     "start_time": "2021-04-26T12:53:58.988000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.052982,
     "end_time": "2021-04-26T12:53:59.066302",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.013320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(text.split(' '))\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for i, s in enumerate(vocab):\n",
    "            self.stoi[s] = i\n",
    "        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n",
    "        \n",
    "    def text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        sequence.append(self.stoi['<sos>'])\n",
    "        for s in text.split(' '):\n",
    "            sequence.append(self.stoi[s])\n",
    "        sequence.append(self.stoi['<eos>'])\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = self.text_to_sequence(text)\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = self.sequence_to_text(sequence)\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    def predict_caption(self, sequence):\n",
    "        caption = ''\n",
    "        for i in sequence:\n",
    "            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n",
    "                break\n",
    "            caption += self.itos[i]\n",
    "        return caption\n",
    "    \n",
    "    def predict_captions(self, sequences):\n",
    "        captions = []\n",
    "        for sequence in sequences:\n",
    "            caption = self.predict_caption(sequence)\n",
    "            captions.append(caption)\n",
    "        return captions\n",
    "\n",
    "tokenizer = torch.load(CFG.prep_path + 'tokenizer.pth')\n",
    "print(f\"tokenizer.stoi: {tokenizer.stoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.028919,
     "end_time": "2021-04-26T12:53:59.109021",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.080102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        score = Levenshtein.distance(true, pred)\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train_aoa_en.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed = CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.027172,
     "end_time": "2021-04-26T12:53:59.149575",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.122403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None):\n",
    "        super().__init__()\n",
    "        self.df         = df\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.labels     = df['InChI_text'].values\n",
    "        self.transform  = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image = image)\n",
    "            image     = augmented['image']\n",
    "        label = self.labels[idx]\n",
    "        label = self.tokenizer.text_to_sequence(label)\n",
    "        label_length = len(label)\n",
    "        label_length = torch.LongTensor([label_length])\n",
    "        return image, torch.LongTensor(label), label_length\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.021826,
     "end_time": "2021-04-26T12:53:59.184877",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.163051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bms_collate(batch):\n",
    "    imgs, labels, label_lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        imgs.append(data_point[0])\n",
    "        labels.append(data_point[1])\n",
    "        label_lengths.append(data_point[2])\n",
    "    labels = pad_sequence(labels, batch_first = True, padding_value = tokenizer.stoi[\"<pad>\"])\n",
    "    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.022625,
     "end_time": "2021-04-26T12:53:59.221171",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.198546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### CNN ENCODER\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name = CFG.model_name, pretrained = False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs       = x.size(0)\n",
    "        features = self.cnn.forward_features(x)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### CNN ENCODER\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, model_name = CFG.model_name, opt = CFG, pretrained = False):\n",
    "#         super().__init__()\n",
    "#         cnn = timm.create_model(model_name, pretrained = False)\n",
    "#         self.cnns = clones(cnn, opt.ensemble_num)\n",
    "#         self.refiner = AoA_Refiner_Core(opt.num_heads, opt.enc_size, opt.aoa_use_ff)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         bs       = x.size(0)\n",
    "#         features = torch.mean(torch.stack([enc.forward_features(x) for enc in self.cnns]), dim=0)\n",
    "#         features = features.permute(0, 2, 3, 1)\n",
    "#         features = features.view(features.shape[0],-1, features.shape[-1])\n",
    "#         features = self.refiner(features, None)\n",
    "#         return features\n",
    "\n",
    "class Refiner(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__()\n",
    "        self.refiner = AoA_Refiner_Core(opt.num_heads, opt.enc_size, opt.aoa_use_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.refiner(x, None)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013651,
     "end_time": "2021-04-26T12:53:59.248675",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.235024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The class `DecoderWithAttention` is updated to support a multi-layer LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.056587,
     "end_time": "2021-04-26T12:53:59.318921",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.262334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### RNN DECODER\n",
    "# attention module\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention network for calculate attention value\n",
    "    '''\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        '''\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param attention_dim: input size of attention network\n",
    "        '''\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n",
    "        self.relu        = nn.ReLU()\n",
    "        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n",
    "        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "    \n",
    "# custom LSTM cell\n",
    "def LSTMCell(input_size, hidden_size, **kwargs):\n",
    "    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# decoder\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    '''\n",
    "    Decoder network with attention network used for training\n",
    "    '''\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers, opt):\n",
    "        '''\n",
    "        :param attention_dim: input size of attention network\n",
    "        :param embed_dim: input size of embedding network\n",
    "        :param decoder_dim: input size of decoder network\n",
    "        :param vocab_size: total number of characters used in training\n",
    "        :param encoder_dim: input size of encoder network\n",
    "        :param num_layers: number of the LSTM layers\n",
    "        :param dropout: dropout rate\n",
    "        '''\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim   = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim     = embed_dim\n",
    "        self.decoder_dim   = decoder_dim\n",
    "        self.vocab_size    = vocab_size\n",
    "        self.dropout       = dropout\n",
    "        self.num_layers    = num_layers\n",
    "        self.device        = device\n",
    "        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n",
    "        self.dropout       = nn.Dropout(p = self.dropout)\n",
    "        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n",
    "        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid       = nn.Sigmoid()\n",
    "        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.refiner = Refiner(opt)\n",
    "        self.init_weights()                                      # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune = True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim = 1)\n",
    "        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n",
    "        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        '''\n",
    "        :param encoder_out: output of encoder network\n",
    "        :param encoded_captions: transformed sequence from character to integer\n",
    "        :param caption_lengths: length of transformed sequence\n",
    "        '''\n",
    "        batch_size       = encoder_out.size(0)\n",
    "        encoder_dim      = encoder_out.size(-1)\n",
    "        vocab_size       = self.vocab_size\n",
    "        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        # encoder_out = encoder_out.mean(dim = 1, keepdim = True)\n",
    "        encoder_out = self.refiner(encoder_out)\n",
    "        num_pixels       = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n",
    "        encoder_out      = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        \n",
    "        # embedding transformed sequence for vector\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        \n",
    "        # Initialize LSTM state, initialize cell_vector and hidden_vector\n",
    "        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        # set decode length by caption length - 1 because of omitting start token\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n",
    "        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                prev_h[-1][:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
    "            \n",
    "            for i, rnn in enumerate(self.decode_step):\n",
    "                # recurrent cell\n",
    "                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n",
    "\n",
    "                # hidden state becomes the input to the next layer\n",
    "                input = self.dropout(h)\n",
    "\n",
    "                # save state for next time step\n",
    "                prev_h[i] = h\n",
    "                prev_c[i] = c\n",
    "                \n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :]      = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def predict(self, encoder_out, decode_lengths, tokenizer):\n",
    "        \n",
    "        # size variables\n",
    "        batch_size  = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size  = self.vocab_size\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        encoder_out = self.refiner(encoder_out)\n",
    "        num_pixels  = encoder_out.size(1)\n",
    "        \n",
    "        # embed start tocken for LSTM input\n",
    "        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n",
    "        embeddings    = self.embedding(start_tockens)\n",
    "        \n",
    "        # initialize hidden state and cell state of LSTM cell\n",
    "        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n",
    "        \n",
    "        # predict sequence\n",
    "        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n",
    "        for t in range(decode_lengths):\n",
    "            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "            awe        = gate * awe\n",
    "            \n",
    "            input = torch.cat([embeddings, awe], dim=1)\n",
    " \n",
    "            for j, rnn in enumerate(self.decode_step):\n",
    "                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "                input = self.dropout(at_h)\n",
    "                h[j]  = at_h\n",
    "                c[j]  = at_c\n",
    "            \n",
    "            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "            predictions[:, t, :] = preds\n",
    "            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n",
    "            if end_condition.sum() == batch_size:\n",
    "                break\n",
    "            embeddings = self.embedding(torch.argmax(preds, -1))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    # beam search\n",
    "    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n",
    "        \n",
    "        h, c = hidden\n",
    "        #h, c = h.squeeze(0), c.squeeze(0)\n",
    "        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n",
    "        \n",
    "        embeddings = self.embedding(prev_tokens)\n",
    "        if embeddings.dim() == 3:\n",
    "            embeddings = embeddings.squeeze(1)\n",
    "            \n",
    "        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n",
    "        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n",
    "        awe        = gate * awe\n",
    "        \n",
    "        input = torch.cat([embeddings, awe], dim = 1)\n",
    "        for j, rnn in enumerate(self.decode_step):\n",
    "            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n",
    "            input = self.dropout(at_h)\n",
    "            h[j]  = at_h\n",
    "            c[j]  = at_c\n",
    "\n",
    "        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n",
    "\n",
    "        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n",
    "        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n",
    "        predicted_softmax = function(preds, dim = 1)\n",
    "        \n",
    "        return predicted_softmax, hidden, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.043018,
     "end_time": "2021-04-26T12:53:59.379363",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.336345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val   = 0\n",
    "        self.avg   = 0\n",
    "        self.sum   = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val    = val\n",
    "        self.sum   += val * n\n",
    "        self.count += n\n",
    "        self.avg    = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s   = now - since\n",
    "    es  = s / (percent)\n",
    "    rs  = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, encoders, decoder, criterion, \n",
    "             encoder_optimizers, decoder_optimizer, epoch,\n",
    "             encoder_schedulers, decoder_scheduler, device):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    losses     = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    # encoder.train()\n",
    "    for encoder in encoders:\n",
    "        encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    for step, (images, labels, label_lengths) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images        = images.to(device)\n",
    "        labels        = labels.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        batch_size    = images.size(0)\n",
    "        \n",
    "        features = torch.mean(torch.stack([enc(images) for enc in encoders]), dim=0)\n",
    "        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n",
    "        targets     = caps_sorted[:, 1:]\n",
    "        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n",
    "        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        loss        = criterion(predictions, targets)\n",
    "        \n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "            \n",
    "        if CFG.apex:\n",
    "            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n",
    "        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        for encoder_scheduler in encoder_schedulers:\n",
    "            encoder_scheduler.step()\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            for encoder_optimizer in encoder_optimizers:\n",
    "                encoder_optimizer.step()\n",
    "                encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.step()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n",
    "                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n",
    "                  'Encoder LR: {encoder_lr:.6f}  '\n",
    "                  'Decoder LR: {decoder_lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), \n",
    "                   batch_time        = batch_time,\n",
    "                   data_time         = data_time, \n",
    "                   loss              = losses,\n",
    "                   remain            = timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   encoder_grad_norm = encoder_grad_norm,\n",
    "                   decoder_grad_norm = decoder_grad_norm,\n",
    "                   encoder_lr=encoder_schedulers[0].get_lr()[0],\n",
    "                   decoder_lr=decoder_scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, encoders, decoder, tokenizer, criterion, device):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    for encoder in encoders:\n",
    "        encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    text_preds = []\n",
    "    start = end = time.time()\n",
    "    \n",
    "    for step, (images) in enumerate(valid_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        images     = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = torch.mean(torch.stack([enc(images) for enc in encoders]), dim=0)\n",
    "            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n",
    "            \n",
    "        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n",
    "        _text_preds        = tokenizer.predict_captions(predicted_sequence)\n",
    "        text_preds.append(_text_preds)\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), \n",
    "                   batch_time = batch_time,\n",
    "                   data_time  = data_time,\n",
    "                   remain     = timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "            \n",
    "    text_preds = np.concatenate(text_preds)\n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.040724,
     "end_time": "2021-04-26T12:53:59.437011",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.396287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds  = folds.loc[trn_idx].reset_index(drop = True)\n",
    "    valid_folds  = folds.loc[val_idx].reset_index(drop = True)\n",
    "    valid_labels = valid_folds['InChI'].values\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, tokenizer, transform = get_transforms(data = 'train'))\n",
    "    valid_dataset = TestDataset(valid_folds, transform = get_transforms(data = 'valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size  = CFG.batch_size, \n",
    "                              shuffle     = True, \n",
    "                              num_workers = CFG.num_workers, \n",
    "                              pin_memory  = True,\n",
    "                              drop_last   = True, \n",
    "                              collate_fn  = bms_collate)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size  = CFG.batch_size, \n",
    "                              shuffle     = False, \n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory  = True, \n",
    "                              drop_last   = False)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                          mode     = 'min', \n",
    "                                          factor   = CFG.factor, \n",
    "                                          patience = CFG.patience, \n",
    "                                          verbose  = True, \n",
    "                                          eps      = CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, \n",
    "                                          T_max      = CFG.T_max, \n",
    "                                          eta_min    = CFG.min_lr, \n",
    "                                          last_epoch = -1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                    T_0        = CFG.T_0, \n",
    "                                                    T_mult     = 1, \n",
    "                                                    eta_min    = CFG.min_lr, \n",
    "                                                    last_epoch = -1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    states = torch.load(CFG.prev_model,  map_location=torch.device('cpu'))\n",
    "    \n",
    "    encoders = [Encoder(CFG.model_name, pretrained = True).to(device) for _ in range(CFG.ensemble_num)]\n",
    "    # for encoder, prev_model in zip(encoders, CFG.prev_models):\n",
    "    #     encoder.load_state_dict(torch.load(prev_model,  map_location=torch.device('cpu'))['encoder'])\n",
    "    # encoder.load_state_dict(states['encoder'])\n",
    "    encoders[0].load_state_dict(torch.load(CFG.prev_model,  map_location=torch.device('cpu'))['encoder0'])\n",
    "    encoders[1].load_state_dict(torch.load(CFG.prev_model,  map_location=torch.device('cpu'))['encoder1'])\n",
    "                                                                    \n",
    "    encoder_optimizers = [Adam(encoders[i].parameters(), \n",
    "                             lr           = CFG.encoder_lr, \n",
    "                             weight_decay = CFG.weight_decay, \n",
    "                             amsgrad      = False) for i in range(CFG.ensemble_num)]\n",
    "#    encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n",
    "    encoder_schedulers = [get_scheduler(optimizer) for optimizer in encoder_optimizers]\n",
    "#    encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n",
    "                                   embed_dim     = CFG.embed_dim, \n",
    "                                   encoder_dim   = CFG.enc_size,\n",
    "                                   decoder_dim   = CFG.decoder_dim,\n",
    "                                   num_layers    = CFG.decoder_layers,\n",
    "                                   vocab_size    = len(tokenizer), \n",
    "                                   dropout       = CFG.dropout, \n",
    "                                   device        = device,\n",
    "                                   opt = CFG)\n",
    "    decoder.load_state_dict(states['decoder'])\n",
    "    decoder.to(device)\n",
    "    decoder_optimizer = Adam(decoder.parameters(), \n",
    "                             lr           = CFG.decoder_lr, \n",
    "                             weight_decay = CFG.weight_decay, \n",
    "                             amsgrad      = False)\n",
    "#    decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n",
    "\n",
    "    decoder_scheduler = get_scheduler(decoder_optimizer)\n",
    " #   decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_loss  = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, encoders, decoder, criterion, \n",
    "                            encoder_optimizers, decoder_optimizer, epoch, \n",
    "                            encoder_schedulers, decoder_scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        text_preds = valid_fn(valid_loader, encoders, decoder, tokenizer, criterion, device)\n",
    "        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n",
    "        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n",
    "        LOGGER.info(f\"preds: {text_preds[:5]}\")\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, text_preds)\n",
    "        \n",
    "        for encoder_scheduler in encoder_schedulers:\n",
    "            encoder_scheduler.step()\n",
    "            \n",
    "        decoder_scheduler.step()\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'encoder0': encoders[0].state_dict(), \n",
    "                        'encoder1': encoders[1].state_dict(), \n",
    "                        'encoder_optimizer0': encoder_optimizers[0].state_dict(), \n",
    "                        'encoder_optimizer1': encoder_optimizers[1].state_dict(), \n",
    "                        'encoder_scheduler0': encoder_schedulers[0].state_dict(), \n",
    "                        'encoder_scheduler1': encoder_schedulers[1].state_dict(), \n",
    "                        'decoder': decoder.state_dict(), \n",
    "                        'decoder_optimizer': decoder_optimizer.state_dict(), \n",
    "                        'decoder_scheduler': decoder_scheduler.state_dict(), \n",
    "                        'text_preds': text_preds,\n",
    "                       },\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_aoa_en.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.021246,
     "end_time": "2021-04-26T12:53:59.473210",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.451964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_file_path(image_id):\n",
    "\n",
    "    return CFG.train_path + \"{}/{}/{}/{}.png\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.023526,
     "end_time": "2021-04-26T12:53:59.512265",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.488739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformations\n",
    "\n",
    "# - train images: `mean = 0.9871, std = 0.0888`\n",
    "# - test images:  `mean = 0.9863, std = 0.0921`\n",
    "\n",
    "train_mean = 1-0.9871\n",
    "train_std = 0.0888\n",
    "test_mean = 1-0.9863\n",
    "test_std = 0.0921\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(CFG.size, CFG.size, scale=(0.95, 1.05), ratio=(1.0, 1.0), p=0.5),\n",
    "            Rotate(limit=2, p=0.5),\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[train_mean, train_mean, train_mean],\n",
    "                std=[train_std, train_std, train_std],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "            \n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[test_mean, test_mean, test_mean],\n",
    "                std=[test_std, test_std, test_std],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01535,
     "end_time": "2021-04-26T12:53:59.543526",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.528176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 13.618271,
     "end_time": "2021-04-26T12:54:13.176857",
     "exception": false,
     "start_time": "2021-04-26T12:53:59.558586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train.shape: (2424186, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_pickle(CFG.prep_path + 'train.pkl')\n",
    "\n",
    "train['file_path'] = train['image_id'].apply(get_train_file_path)\n",
    "\n",
    "print(f'train.shape: {train.shape}')\n",
    "\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n = CFG.samp_size, random_state = CFG.seed).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.093227,
     "end_time": "2021-04-26T12:54:13.285370",
     "exception": false,
     "start_time": "2021-04-26T12:54:13.192143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(train, tokenizer, transform = get_transforms(data='train'))\n",
    "\n",
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "def visualize_data():\n",
    "    for i in range(3):\n",
    "        image, label, label_length = train_dataset[i]\n",
    "        text = tokenizer.sequence_to_text(label.numpy())\n",
    "        figure(figsize=(30, 15), dpi=200)\n",
    "        plt.imshow(image.transpose(0, 1).transpose(1, 2))\n",
    "        plt.title(f'label: {label}  text: {text}  label_length: {label_length}')\n",
    "        plt.savefig(str(i) + '.png')\n",
    "        plt.show()\n",
    "#visualize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014371,
     "end_time": "2021-04-26T12:54:13.314571",
     "exception": false,
     "start_time": "2021-04-26T12:54:13.300200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 2393.0926,
     "end_time": "2021-04-26T13:34:06.421981",
     "exception": false,
     "start_time": "2021-04-26T12:54:13.329381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "========== fold: 0 training ==========\n",
      "Epoch: [1][0/57574] Data 2.299 (2.299) Elapsed 0m 3s (remain 3390m 56s) Loss: 0.7076(0.7076) Encoder Grad: 0.0605  Decoder Grad: 0.2059  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][100/57574] Data 0.000 (0.023) Elapsed 1m 19s (remain 750m 46s) Loss: 0.6085(0.5780) Encoder Grad: 0.4037  Decoder Grad: 0.6396  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][200/57574] Data 0.000 (0.012) Elapsed 2m 34s (remain 733m 6s) Loss: 0.6785(0.5739) Encoder Grad: 0.0449  Decoder Grad: 0.1563  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][300/57574] Data 0.000 (0.008) Elapsed 3m 50s (remain 730m 16s) Loss: 0.5168(0.5695) Encoder Grad: 0.1283  Decoder Grad: 0.1527  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][400/57574] Data 0.000 (0.006) Elapsed 5m 6s (remain 727m 25s) Loss: 0.5160(0.5659) Encoder Grad: 0.1183  Decoder Grad: 0.1990  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][500/57574] Data 0.000 (0.005) Elapsed 6m 21s (remain 724m 44s) Loss: 0.5273(0.5656) Encoder Grad: 0.1466  Decoder Grad: 0.1593  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][600/57574] Data 0.000 (0.004) Elapsed 7m 38s (remain 723m 37s) Loss: 0.5663(0.5705) Encoder Grad: 0.1746  Decoder Grad: 0.2121  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][700/57574] Data 0.000 (0.003) Elapsed 8m 50s (remain 717m 33s) Loss: 0.6152(0.5764) Encoder Grad: 0.1677  Decoder Grad: 0.1777  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][800/57574] Data 0.000 (0.003) Elapsed 10m 3s (remain 712m 42s) Loss: 0.6188(0.5803) Encoder Grad: 0.0223  Decoder Grad: 0.1377  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][900/57574] Data 0.000 (0.003) Elapsed 11m 15s (remain 708m 31s) Loss: 0.5516(0.5818) Encoder Grad: 0.0571  Decoder Grad: 0.1531  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1000/57574] Data 0.000 (0.003) Elapsed 12m 30s (remain 706m 43s) Loss: 0.5133(0.5823) Encoder Grad: 0.2953  Decoder Grad: 0.4346  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1100/57574] Data 0.000 (0.002) Elapsed 13m 43s (remain 704m 4s) Loss: 0.5290(0.5810) Encoder Grad: 1.0199  Decoder Grad: 0.5387  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1200/57574] Data 0.000 (0.002) Elapsed 14m 59s (remain 703m 18s) Loss: 0.6408(0.5810) Encoder Grad: 0.1321  Decoder Grad: 0.2282  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1300/57574] Data 0.000 (0.002) Elapsed 16m 11s (remain 700m 39s) Loss: 0.6168(0.5817) Encoder Grad: 0.0601  Decoder Grad: 0.1435  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1400/57574] Data 0.000 (0.002) Elapsed 17m 26s (remain 699m 21s) Loss: 0.6196(0.5835) Encoder Grad: 0.1156  Decoder Grad: 0.1753  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1500/57574] Data 0.000 (0.002) Elapsed 18m 40s (remain 697m 28s) Loss: 0.6309(0.5857) Encoder Grad: 0.1311  Decoder Grad: 0.1833  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1600/57574] Data 0.000 (0.002) Elapsed 19m 54s (remain 696m 15s) Loss: 0.5953(0.5868) Encoder Grad: 0.2073  Decoder Grad: 0.4089  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1700/57574] Data 0.000 (0.002) Elapsed 21m 9s (remain 694m 58s) Loss: 0.6044(0.5877) Encoder Grad: 0.0680  Decoder Grad: 0.1867  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1800/57574] Data 0.000 (0.001) Elapsed 22m 23s (remain 693m 35s) Loss: 0.6327(0.5882) Encoder Grad: 0.0448  Decoder Grad: 0.1396  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][1900/57574] Data 0.000 (0.001) Elapsed 23m 38s (remain 692m 15s) Loss: 0.6065(0.5897) Encoder Grad: 0.1876  Decoder Grad: 0.1720  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2000/57574] Data 0.000 (0.001) Elapsed 24m 52s (remain 690m 39s) Loss: 0.5534(0.5902) Encoder Grad: 0.1400  Decoder Grad: 0.3288  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2100/57574] Data 0.000 (0.001) Elapsed 26m 6s (remain 689m 16s) Loss: 0.6108(0.5912) Encoder Grad: 0.0954  Decoder Grad: 0.2136  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2200/57574] Data 0.000 (0.001) Elapsed 27m 20s (remain 687m 46s) Loss: 0.6091(0.5922) Encoder Grad: 0.2434  Decoder Grad: 0.3831  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2300/57574] Data 0.000 (0.001) Elapsed 28m 34s (remain 686m 22s) Loss: 0.6560(0.5928) Encoder Grad: 0.0452  Decoder Grad: 0.1414  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2400/57574] Data 0.000 (0.001) Elapsed 29m 48s (remain 685m 3s) Loss: 0.5509(0.5935) Encoder Grad: 0.2862  Decoder Grad: 0.1616  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2500/57574] Data 0.000 (0.001) Elapsed 31m 4s (remain 684m 9s) Loss: 0.5823(0.5940) Encoder Grad: 1.9352  Decoder Grad: 0.7962  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2600/57574] Data 0.000 (0.001) Elapsed 32m 18s (remain 682m 58s) Loss: 0.6346(0.5946) Encoder Grad: 0.1177  Decoder Grad: 0.1538  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2700/57574] Data 0.000 (0.001) Elapsed 33m 33s (remain 681m 47s) Loss: 0.6454(0.5948) Encoder Grad: 0.1053  Decoder Grad: 0.1897  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2800/57574] Data 0.000 (0.001) Elapsed 34m 47s (remain 680m 16s) Loss: 0.5662(0.5950) Encoder Grad: 0.0602  Decoder Grad: 0.1960  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][2900/57574] Data 0.000 (0.001) Elapsed 36m 1s (remain 678m 51s) Loss: 0.5142(0.5951) Encoder Grad: 0.7853  Decoder Grad: 0.3635  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3000/57574] Data 0.000 (0.001) Elapsed 37m 15s (remain 677m 37s) Loss: 0.5771(0.5952) Encoder Grad: 0.4013  Decoder Grad: 0.1964  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3100/57574] Data 0.000 (0.001) Elapsed 38m 30s (remain 676m 21s) Loss: 0.5675(0.5952) Encoder Grad: 0.1286  Decoder Grad: 0.2364  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3200/57574] Data 0.000 (0.001) Elapsed 39m 44s (remain 675m 3s) Loss: 0.6003(0.5954) Encoder Grad: 0.2343  Decoder Grad: 0.2151  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3300/57574] Data 0.000 (0.001) Elapsed 40m 59s (remain 673m 51s) Loss: 0.6343(0.5957) Encoder Grad: 0.8068  Decoder Grad: 0.2785  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3400/57574] Data 0.000 (0.001) Elapsed 42m 13s (remain 672m 39s) Loss: 0.5648(0.5957) Encoder Grad: 0.1293  Decoder Grad: 0.1823  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3500/57574] Data 0.000 (0.001) Elapsed 43m 28s (remain 671m 25s) Loss: 0.7003(0.5961) Encoder Grad: 0.2769  Decoder Grad: 0.2971  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3600/57574] Data 0.000 (0.001) Elapsed 44m 44s (remain 670m 30s) Loss: 0.5433(0.5965) Encoder Grad: 0.0613  Decoder Grad: 0.2060  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3700/57574] Data 0.000 (0.001) Elapsed 45m 58s (remain 669m 16s) Loss: 0.5703(0.5964) Encoder Grad: 0.0573  Decoder Grad: 0.1339  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3800/57574] Data 0.000 (0.001) Elapsed 47m 13s (remain 668m 0s) Loss: 0.6049(0.5965) Encoder Grad: 0.1234  Decoder Grad: 0.3048  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][3900/57574] Data 0.000 (0.001) Elapsed 48m 27s (remain 666m 49s) Loss: 0.6439(0.5966) Encoder Grad: 0.0999  Decoder Grad: 0.1738  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4000/57574] Data 0.000 (0.001) Elapsed 49m 42s (remain 665m 32s) Loss: 0.5954(0.5965) Encoder Grad: 0.0660  Decoder Grad: 0.1597  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4100/57574] Data 0.000 (0.001) Elapsed 50m 56s (remain 664m 18s) Loss: 0.5933(0.5967) Encoder Grad: 0.0639  Decoder Grad: 0.1524  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4200/57574] Data 0.000 (0.001) Elapsed 52m 11s (remain 663m 2s) Loss: 0.5724(0.5966) Encoder Grad: 0.1175  Decoder Grad: 0.2696  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4300/57574] Data 0.000 (0.001) Elapsed 53m 24s (remain 661m 34s) Loss: 0.5662(0.5965) Encoder Grad: 0.0216  Decoder Grad: 0.1414  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4400/57574] Data 0.000 (0.001) Elapsed 54m 39s (remain 660m 25s) Loss: 0.7105(0.5966) Encoder Grad: 0.0522  Decoder Grad: 0.1573  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4500/57574] Data 0.000 (0.001) Elapsed 55m 54s (remain 659m 9s) Loss: 0.5777(0.5965) Encoder Grad: 0.1463  Decoder Grad: 0.1533  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4600/57574] Data 0.000 (0.001) Elapsed 57m 8s (remain 657m 53s) Loss: 0.6891(0.5965) Encoder Grad: 0.0811  Decoder Grad: 0.1774  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4700/57574] Data 0.000 (0.001) Elapsed 58m 22s (remain 656m 35s) Loss: 0.5611(0.5966) Encoder Grad: 0.0271  Decoder Grad: 0.1487  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4800/57574] Data 0.000 (0.001) Elapsed 59m 37s (remain 655m 24s) Loss: 0.5904(0.5969) Encoder Grad: 0.2921  Decoder Grad: 0.2350  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][4900/57574] Data 0.000 (0.001) Elapsed 60m 52s (remain 654m 11s) Loss: 0.5990(0.5970) Encoder Grad: 0.0276  Decoder Grad: 0.1418  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5000/57574] Data 0.000 (0.001) Elapsed 62m 6s (remain 652m 57s) Loss: 0.5385(0.5968) Encoder Grad: 0.1182  Decoder Grad: 0.2692  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5100/57574] Data 0.000 (0.001) Elapsed 63m 21s (remain 651m 44s) Loss: 0.6091(0.5969) Encoder Grad: 0.0937  Decoder Grad: 0.1628  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5200/57574] Data 0.000 (0.001) Elapsed 64m 35s (remain 650m 29s) Loss: 0.5456(0.5967) Encoder Grad: 0.1070  Decoder Grad: 0.1505  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5300/57574] Data 0.000 (0.001) Elapsed 65m 50s (remain 649m 14s) Loss: 0.5202(0.5963) Encoder Grad: 0.0937  Decoder Grad: 0.2251  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5400/57574] Data 0.000 (0.001) Elapsed 67m 4s (remain 647m 57s) Loss: 0.5793(0.5960) Encoder Grad: 0.1386  Decoder Grad: 0.2299  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5500/57574] Data 0.000 (0.001) Elapsed 68m 19s (remain 646m 42s) Loss: 0.5982(0.5958) Encoder Grad: 0.0572  Decoder Grad: 0.1618  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5600/57574] Data 0.000 (0.001) Elapsed 69m 32s (remain 645m 21s) Loss: 0.6436(0.5958) Encoder Grad: 0.1472  Decoder Grad: 0.2585  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5700/57574] Data 0.000 (0.001) Elapsed 70m 47s (remain 644m 6s) Loss: 0.6310(0.5958) Encoder Grad: 0.0726  Decoder Grad: 0.1615  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5800/57574] Data 0.000 (0.001) Elapsed 72m 2s (remain 642m 53s) Loss: 0.5713(0.5957) Encoder Grad: 0.0382  Decoder Grad: 0.1618  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][5900/57574] Data 0.000 (0.001) Elapsed 73m 15s (remain 641m 32s) Loss: 0.5389(0.5954) Encoder Grad: 0.2140  Decoder Grad: 0.2659  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6000/57574] Data 0.000 (0.001) Elapsed 74m 29s (remain 640m 13s) Loss: 0.5488(0.5952) Encoder Grad: 0.4080  Decoder Grad: 0.2780  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6100/57574] Data 0.000 (0.001) Elapsed 75m 44s (remain 638m 59s) Loss: 0.5983(0.5947) Encoder Grad: 0.1164  Decoder Grad: 0.1601  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6200/57574] Data 0.000 (0.001) Elapsed 76m 58s (remain 637m 40s) Loss: 0.6221(0.5945) Encoder Grad: 0.3158  Decoder Grad: 0.2124  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6300/57574] Data 0.000 (0.001) Elapsed 78m 12s (remain 636m 24s) Loss: 0.6934(0.5942) Encoder Grad: 0.1539  Decoder Grad: 0.1925  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6400/57574] Data 0.000 (0.001) Elapsed 79m 24s (remain 634m 51s) Loss: 0.6301(0.5938) Encoder Grad: 0.1715  Decoder Grad: 0.1904  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6500/57574] Data 0.000 (0.001) Elapsed 80m 31s (remain 632m 40s) Loss: 0.6400(0.5933) Encoder Grad: 0.0791  Decoder Grad: 0.1699  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6600/57574] Data 0.000 (0.001) Elapsed 81m 40s (remain 630m 38s) Loss: 0.5378(0.5929) Encoder Grad: 0.1336  Decoder Grad: 0.3794  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6700/57574] Data 0.000 (0.001) Elapsed 82m 51s (remain 629m 6s) Loss: 0.5102(0.5925) Encoder Grad: 0.1229  Decoder Grad: 0.2399  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6800/57574] Data 0.000 (0.001) Elapsed 84m 7s (remain 628m 5s) Loss: 0.6037(0.5922) Encoder Grad: 0.0750  Decoder Grad: 0.2038  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][6900/57574] Data 0.000 (0.001) Elapsed 85m 24s (remain 627m 7s) Loss: 0.5376(0.5917) Encoder Grad: 0.1468  Decoder Grad: 0.1864  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7000/57574] Data 0.000 (0.001) Elapsed 86m 39s (remain 626m 1s) Loss: 0.5935(0.5914) Encoder Grad: 0.0803  Decoder Grad: 0.1792  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7100/57574] Data 0.000 (0.001) Elapsed 87m 53s (remain 624m 42s) Loss: 0.5469(0.5911) Encoder Grad: 0.0594  Decoder Grad: 0.1720  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7200/57574] Data 0.000 (0.001) Elapsed 89m 8s (remain 623m 34s) Loss: 0.5355(0.5906) Encoder Grad: 0.0828  Decoder Grad: 0.1555  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7300/57574] Data 0.000 (0.001) Elapsed 90m 22s (remain 622m 19s) Loss: 0.5160(0.5902) Encoder Grad: 0.0668  Decoder Grad: 0.1830  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7400/57574] Data 0.000 (0.001) Elapsed 91m 37s (remain 621m 10s) Loss: 0.5778(0.5898) Encoder Grad: 0.0479  Decoder Grad: 0.1554  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7500/57574] Data 0.000 (0.001) Elapsed 92m 53s (remain 620m 3s) Loss: 0.5150(0.5894) Encoder Grad: 0.0598  Decoder Grad: 0.1289  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7600/57574] Data 0.000 (0.001) Elapsed 94m 8s (remain 618m 55s) Loss: 0.5724(0.5890) Encoder Grad: 0.1409  Decoder Grad: 0.1683  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7700/57574] Data 0.000 (0.001) Elapsed 95m 23s (remain 617m 48s) Loss: 0.5578(0.5885) Encoder Grad: 0.0397  Decoder Grad: 0.1518  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7800/57574] Data 0.000 (0.000) Elapsed 96m 41s (remain 616m 54s) Loss: 0.7048(0.5880) Encoder Grad: 0.0547  Decoder Grad: 0.1791  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][7900/57574] Data 0.000 (0.000) Elapsed 97m 57s (remain 615m 49s) Loss: 0.5482(0.5876) Encoder Grad: 0.7751  Decoder Grad: 0.2763  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8000/57574] Data 0.000 (0.000) Elapsed 99m 13s (remain 614m 47s) Loss: 0.5419(0.5871) Encoder Grad: 0.0629  Decoder Grad: 0.1829  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8100/57574] Data 0.000 (0.000) Elapsed 100m 29s (remain 613m 40s) Loss: 0.5249(0.5867) Encoder Grad: 0.0741  Decoder Grad: 0.1755  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8200/57574] Data 0.000 (0.000) Elapsed 101m 45s (remain 612m 36s) Loss: 0.5339(0.5863) Encoder Grad: 0.1163  Decoder Grad: 0.1705  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8300/57574] Data 0.000 (0.000) Elapsed 103m 1s (remain 611m 34s) Loss: 0.6074(0.5859) Encoder Grad: 0.0789  Decoder Grad: 0.1870  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8400/57574] Data 0.000 (0.000) Elapsed 104m 18s (remain 610m 29s) Loss: 0.5422(0.5855) Encoder Grad: 0.1685  Decoder Grad: 0.5476  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8500/57574] Data 0.000 (0.000) Elapsed 105m 34s (remain 609m 24s) Loss: 0.6009(0.5851) Encoder Grad: 0.3099  Decoder Grad: 0.7935  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8600/57574] Data 0.000 (0.000) Elapsed 106m 51s (remain 608m 27s) Loss: 0.5232(0.5847) Encoder Grad: 0.0754  Decoder Grad: 0.2248  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8700/57574] Data 0.000 (0.000) Elapsed 108m 7s (remain 607m 21s) Loss: 0.6114(0.5842) Encoder Grad: 0.0779  Decoder Grad: 0.1585  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8800/57574] Data 0.000 (0.000) Elapsed 109m 23s (remain 606m 14s) Loss: 0.5091(0.5837) Encoder Grad: 0.0768  Decoder Grad: 0.1777  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][8900/57574] Data 0.000 (0.000) Elapsed 110m 39s (remain 605m 8s) Loss: 0.5202(0.5833) Encoder Grad: 0.0400  Decoder Grad: 0.1589  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9000/57574] Data 0.000 (0.000) Elapsed 111m 56s (remain 604m 3s) Loss: 0.6727(0.5831) Encoder Grad: 0.0491  Decoder Grad: 0.1777  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9100/57574] Data 0.000 (0.000) Elapsed 113m 12s (remain 602m 57s) Loss: 0.5815(0.5829) Encoder Grad: 0.1716  Decoder Grad: 0.1835  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9200/57574] Data 0.000 (0.000) Elapsed 114m 28s (remain 601m 49s) Loss: 0.5173(0.5825) Encoder Grad: 0.1180  Decoder Grad: 0.2531  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9300/57574] Data 0.000 (0.000) Elapsed 115m 46s (remain 600m 53s) Loss: 0.5307(0.5821) Encoder Grad: 0.1676  Decoder Grad: 0.2327  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9400/57574] Data 0.000 (0.000) Elapsed 117m 4s (remain 599m 52s) Loss: 0.5484(0.5818) Encoder Grad: 0.0879  Decoder Grad: 0.2439  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9500/57574] Data 0.000 (0.000) Elapsed 118m 20s (remain 598m 48s) Loss: 0.5283(0.5814) Encoder Grad: 0.0537  Decoder Grad: 0.1736  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9600/57574] Data 0.000 (0.000) Elapsed 119m 37s (remain 597m 44s) Loss: 0.5089(0.5810) Encoder Grad: 0.0487  Decoder Grad: 0.1803  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9700/57574] Data 0.000 (0.000) Elapsed 120m 52s (remain 596m 32s) Loss: 0.5340(0.5807) Encoder Grad: 0.0577  Decoder Grad: 0.2072  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9800/57574] Data 0.000 (0.000) Elapsed 122m 9s (remain 595m 28s) Loss: 0.5814(0.5804) Encoder Grad: 0.0431  Decoder Grad: 0.1863  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][9900/57574] Data 0.000 (0.000) Elapsed 123m 25s (remain 594m 15s) Loss: 0.5488(0.5803) Encoder Grad: 0.8036  Decoder Grad: 1.1608  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10000/57574] Data 0.000 (0.000) Elapsed 124m 36s (remain 592m 42s) Loss: 0.5315(0.5801) Encoder Grad: 0.3966  Decoder Grad: 0.2137  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10100/57574] Data 0.000 (0.000) Elapsed 125m 46s (remain 591m 9s) Loss: 0.5448(0.5799) Encoder Grad: 0.2204  Decoder Grad: 0.2109  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10200/57574] Data 0.000 (0.000) Elapsed 127m 2s (remain 589m 56s) Loss: 0.4923(0.5796) Encoder Grad: 0.1381  Decoder Grad: 0.2697  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10300/57574] Data 0.000 (0.000) Elapsed 128m 13s (remain 588m 25s) Loss: 0.5986(0.5793) Encoder Grad: 0.1442  Decoder Grad: 0.2304  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10400/57574] Data 0.000 (0.000) Elapsed 129m 28s (remain 587m 11s) Loss: 0.5742(0.5790) Encoder Grad: 0.0489  Decoder Grad: 0.1653  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10500/57574] Data 0.000 (0.000) Elapsed 130m 43s (remain 586m 1s) Loss: 0.5537(0.5787) Encoder Grad: 0.3967  Decoder Grad: 0.3919  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10600/57574] Data 0.000 (0.000) Elapsed 131m 58s (remain 584m 47s) Loss: 0.6025(0.5784) Encoder Grad: 0.1960  Decoder Grad: 0.1718  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10700/57574] Data 0.000 (0.000) Elapsed 133m 14s (remain 583m 38s) Loss: 0.6644(0.5781) Encoder Grad: 0.2817  Decoder Grad: 0.2342  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10800/57574] Data 0.000 (0.000) Elapsed 134m 27s (remain 582m 15s) Loss: 0.6201(0.5779) Encoder Grad: 0.0746  Decoder Grad: 0.1530  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][10900/57574] Data 0.000 (0.000) Elapsed 135m 39s (remain 580m 50s) Loss: 0.5318(0.5776) Encoder Grad: 0.2428  Decoder Grad: 0.1808  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11000/57574] Data 0.000 (0.000) Elapsed 136m 51s (remain 579m 23s) Loss: 0.5961(0.5773) Encoder Grad: 0.1118  Decoder Grad: 0.2004  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11100/57574] Data 0.000 (0.000) Elapsed 138m 4s (remain 578m 0s) Loss: 0.6084(0.5771) Encoder Grad: 0.0545  Decoder Grad: 0.2189  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11200/57574] Data 0.000 (0.000) Elapsed 139m 15s (remain 576m 34s) Loss: 0.5551(0.5768) Encoder Grad: 0.0786  Decoder Grad: 0.1795  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11300/57574] Data 0.000 (0.000) Elapsed 140m 28s (remain 575m 10s) Loss: 0.5714(0.5766) Encoder Grad: 0.1690  Decoder Grad: 0.3581  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11400/57574] Data 0.000 (0.000) Elapsed 141m 40s (remain 573m 45s) Loss: 0.5044(0.5763) Encoder Grad: 0.0537  Decoder Grad: 0.1857  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11500/57574] Data 0.000 (0.000) Elapsed 142m 52s (remain 572m 23s) Loss: 0.5618(0.5762) Encoder Grad: 0.1003  Decoder Grad: 0.1851  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11600/57574] Data 0.000 (0.000) Elapsed 144m 5s (remain 570m 59s) Loss: 0.5817(0.5762) Encoder Grad: 0.1710  Decoder Grad: 0.2827  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11700/57574] Data 0.000 (0.000) Elapsed 145m 20s (remain 569m 47s) Loss: 0.6073(0.5761) Encoder Grad: 0.1068  Decoder Grad: 0.1936  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11800/57574] Data 0.000 (0.000) Elapsed 146m 34s (remain 568m 31s) Loss: 0.5545(0.5761) Encoder Grad: 0.1016  Decoder Grad: 0.1706  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][11900/57574] Data 0.000 (0.000) Elapsed 147m 50s (remain 567m 24s) Loss: 0.6111(0.5761) Encoder Grad: 0.0421  Decoder Grad: 0.1640  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12000/57574] Data 0.000 (0.000) Elapsed 149m 7s (remain 566m 15s) Loss: 0.5537(0.5761) Encoder Grad: 0.1356  Decoder Grad: 0.1731  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12100/57574] Data 0.000 (0.000) Elapsed 150m 23s (remain 565m 7s) Loss: 0.5591(0.5761) Encoder Grad: 0.4472  Decoder Grad: 0.7777  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12200/57574] Data 0.000 (0.000) Elapsed 151m 39s (remain 564m 0s) Loss: 0.5205(0.5761) Encoder Grad: 0.1183  Decoder Grad: 0.1669  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12300/57574] Data 0.000 (0.000) Elapsed 152m 56s (remain 562m 52s) Loss: 0.5789(0.5761) Encoder Grad: 0.2500  Decoder Grad: 0.2507  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12400/57574] Data 0.000 (0.000) Elapsed 154m 12s (remain 561m 45s) Loss: 0.5490(0.5760) Encoder Grad: 0.0675  Decoder Grad: 0.1728  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12500/57574] Data 0.000 (0.000) Elapsed 155m 27s (remain 560m 31s) Loss: 0.5960(0.5759) Encoder Grad: 0.0358  Decoder Grad: 0.1600  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12600/57574] Data 0.000 (0.000) Elapsed 156m 44s (remain 559m 24s) Loss: 0.6048(0.5758) Encoder Grad: 0.1012  Decoder Grad: 0.1707  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12700/57574] Data 0.000 (0.000) Elapsed 157m 59s (remain 558m 12s) Loss: 0.5610(0.5758) Encoder Grad: 0.5799  Decoder Grad: 0.3504  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12800/57574] Data 0.000 (0.000) Elapsed 159m 15s (remain 557m 2s) Loss: 0.5243(0.5757) Encoder Grad: 0.1256  Decoder Grad: 0.1786  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][12900/57574] Data 0.000 (0.000) Elapsed 160m 32s (remain 555m 56s) Loss: 0.5507(0.5757) Encoder Grad: 0.1615  Decoder Grad: 0.3139  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13000/57574] Data 0.000 (0.000) Elapsed 161m 47s (remain 554m 41s) Loss: 0.5130(0.5756) Encoder Grad: 0.2008  Decoder Grad: 0.1745  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13100/57574] Data 0.000 (0.000) Elapsed 163m 0s (remain 553m 21s) Loss: 0.5361(0.5754) Encoder Grad: 0.0456  Decoder Grad: 0.1667  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13200/57574] Data 0.000 (0.000) Elapsed 164m 14s (remain 552m 4s) Loss: 0.5261(0.5752) Encoder Grad: 0.0563  Decoder Grad: 0.1940  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13300/57574] Data 0.000 (0.000) Elapsed 165m 32s (remain 551m 0s) Loss: 0.5139(0.5751) Encoder Grad: 0.0411  Decoder Grad: 0.1788  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13400/57574] Data 0.000 (0.000) Elapsed 166m 47s (remain 549m 47s) Loss: 0.5547(0.5749) Encoder Grad: 0.0922  Decoder Grad: 0.1559  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13500/57574] Data 0.000 (0.000) Elapsed 168m 3s (remain 548m 37s) Loss: 0.5748(0.5747) Encoder Grad: 0.0533  Decoder Grad: 0.1905  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13600/57574] Data 0.000 (0.000) Elapsed 169m 21s (remain 547m 31s) Loss: 0.5522(0.5746) Encoder Grad: 0.0363  Decoder Grad: 0.1476  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13700/57574] Data 0.000 (0.000) Elapsed 170m 34s (remain 546m 12s) Loss: 0.5416(0.5746) Encoder Grad: 0.5036  Decoder Grad: 0.5522  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13800/57574] Data 0.000 (0.000) Elapsed 171m 47s (remain 544m 51s) Loss: 0.5706(0.5744) Encoder Grad: 0.1056  Decoder Grad: 0.2118  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][13900/57574] Data 0.000 (0.000) Elapsed 172m 57s (remain 543m 23s) Loss: 0.5488(0.5742) Encoder Grad: 0.0738  Decoder Grad: 0.2242  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14000/57574] Data 0.000 (0.000) Elapsed 174m 6s (remain 541m 49s) Loss: 0.5246(0.5741) Encoder Grad: 0.2666  Decoder Grad: 0.2084  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14100/57574] Data 0.000 (0.000) Elapsed 175m 14s (remain 540m 15s) Loss: 0.5561(0.5740) Encoder Grad: 0.0739  Decoder Grad: 0.2133  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14200/57574] Data 0.000 (0.000) Elapsed 176m 26s (remain 538m 52s) Loss: 0.5595(0.5739) Encoder Grad: 0.8681  Decoder Grad: 0.7615  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14300/57574] Data 0.000 (0.000) Elapsed 177m 37s (remain 537m 29s) Loss: 0.5876(0.5740) Encoder Grad: 0.0296  Decoder Grad: 0.1590  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14400/57574] Data 0.000 (0.000) Elapsed 178m 49s (remain 536m 7s) Loss: 0.6475(0.5739) Encoder Grad: 0.0568  Decoder Grad: 0.1869  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14500/57574] Data 0.000 (0.000) Elapsed 180m 4s (remain 534m 53s) Loss: 0.5875(0.5738) Encoder Grad: 0.1500  Decoder Grad: 0.1652  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14600/57574] Data 0.000 (0.000) Elapsed 181m 18s (remain 533m 36s) Loss: 0.6156(0.5737) Encoder Grad: 0.0629  Decoder Grad: 0.1667  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14700/57574] Data 0.000 (0.000) Elapsed 182m 31s (remain 532m 18s) Loss: 0.5860(0.5736) Encoder Grad: 0.0686  Decoder Grad: 0.1593  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14800/57574] Data 0.000 (0.000) Elapsed 183m 46s (remain 531m 3s) Loss: 0.5784(0.5734) Encoder Grad: 0.0829  Decoder Grad: 0.1706  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][14900/57574] Data 0.000 (0.000) Elapsed 184m 59s (remain 529m 47s) Loss: 0.5031(0.5732) Encoder Grad: 0.2810  Decoder Grad: 0.2638  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15000/57574] Data 0.000 (0.000) Elapsed 186m 15s (remain 528m 37s) Loss: 0.5727(0.5730) Encoder Grad: 0.0324  Decoder Grad: 0.1563  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15100/57574] Data 0.000 (0.000) Elapsed 187m 29s (remain 527m 21s) Loss: 0.4984(0.5728) Encoder Grad: 0.2730  Decoder Grad: 0.2959  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15200/57574] Data 0.000 (0.000) Elapsed 188m 44s (remain 526m 6s) Loss: 0.5003(0.5726) Encoder Grad: 0.6664  Decoder Grad: 0.3409  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15300/57574] Data 0.000 (0.000) Elapsed 190m 3s (remain 525m 4s) Loss: 0.5296(0.5724) Encoder Grad: 0.0411  Decoder Grad: 0.1905  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15400/57574] Data 0.000 (0.000) Elapsed 191m 19s (remain 523m 54s) Loss: 0.5673(0.5723) Encoder Grad: 0.0862  Decoder Grad: 0.1948  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15500/57574] Data 0.000 (0.000) Elapsed 192m 37s (remain 522m 50s) Loss: 0.5019(0.5722) Encoder Grad: 0.0563  Decoder Grad: 0.1673  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15600/57574] Data 0.000 (0.000) Elapsed 193m 54s (remain 521m 40s) Loss: 0.5039(0.5722) Encoder Grad: 0.0990  Decoder Grad: 0.1933  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15700/57574] Data 0.000 (0.000) Elapsed 195m 12s (remain 520m 36s) Loss: 0.6351(0.5721) Encoder Grad: 0.0550  Decoder Grad: 0.1688  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15800/57574] Data 0.000 (0.000) Elapsed 196m 30s (remain 519m 31s) Loss: 0.5418(0.5721) Encoder Grad: 1.4149  Decoder Grad: 0.4274  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][15900/57574] Data 0.000 (0.000) Elapsed 197m 47s (remain 518m 21s) Loss: 0.5854(0.5720) Encoder Grad: 0.1022  Decoder Grad: 0.2543  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16000/57574] Data 0.000 (0.000) Elapsed 198m 57s (remain 516m 56s) Loss: 0.4958(0.5718) Encoder Grad: 0.0456  Decoder Grad: 0.1623  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16100/57574] Data 0.000 (0.000) Elapsed 200m 11s (remain 515m 39s) Loss: 0.4883(0.5716) Encoder Grad: 0.1117  Decoder Grad: 0.2073  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16200/57574] Data 0.000 (0.000) Elapsed 201m 21s (remain 514m 13s) Loss: 0.4921(0.5715) Encoder Grad: 0.2553  Decoder Grad: 0.2425  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16300/57574] Data 0.000 (0.000) Elapsed 202m 35s (remain 512m 57s) Loss: 0.5883(0.5713) Encoder Grad: 0.1732  Decoder Grad: 0.2275  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16400/57574] Data 0.000 (0.000) Elapsed 203m 45s (remain 511m 31s) Loss: 0.5867(0.5711) Encoder Grad: 0.0725  Decoder Grad: 0.1975  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16500/57574] Data 0.000 (0.000) Elapsed 204m 56s (remain 510m 6s) Loss: 0.6531(0.5711) Encoder Grad: 0.0570  Decoder Grad: 0.1765  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16600/57574] Data 0.000 (0.000) Elapsed 206m 12s (remain 508m 55s) Loss: 0.5598(0.5711) Encoder Grad: 0.0387  Decoder Grad: 0.1677  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16700/57574] Data 0.000 (0.000) Elapsed 207m 22s (remain 507m 30s) Loss: 0.5541(0.5711) Encoder Grad: 0.1119  Decoder Grad: 0.1594  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16800/57574] Data 0.000 (0.000) Elapsed 208m 33s (remain 506m 8s) Loss: 0.6191(0.5710) Encoder Grad: 0.0643  Decoder Grad: 0.2038  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][16900/57574] Data 0.000 (0.000) Elapsed 209m 49s (remain 504m 56s) Loss: 0.5219(0.5709) Encoder Grad: 0.0485  Decoder Grad: 0.1700  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17000/57574] Data 0.000 (0.000) Elapsed 211m 1s (remain 503m 37s) Loss: 0.5213(0.5708) Encoder Grad: 0.6977  Decoder Grad: 0.3183  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17100/57574] Data 0.000 (0.000) Elapsed 212m 11s (remain 502m 12s) Loss: 0.5416(0.5707) Encoder Grad: 0.0211  Decoder Grad: 0.1708  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17200/57574] Data 0.000 (0.000) Elapsed 213m 25s (remain 500m 55s) Loss: 0.5630(0.5706) Encoder Grad: 0.2112  Decoder Grad: 0.2365  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17300/57574] Data 0.000 (0.000) Elapsed 214m 36s (remain 499m 33s) Loss: 0.5335(0.5704) Encoder Grad: 0.0509  Decoder Grad: 0.1774  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17400/57574] Data 0.000 (0.000) Elapsed 215m 49s (remain 498m 15s) Loss: 0.5107(0.5703) Encoder Grad: 0.1117  Decoder Grad: 0.3896  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17500/57574] Data 0.000 (0.000) Elapsed 217m 3s (remain 497m 1s) Loss: 0.5522(0.5700) Encoder Grad: 0.3862  Decoder Grad: 0.1877  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17600/57574] Data 0.000 (0.000) Elapsed 218m 13s (remain 495m 37s) Loss: 0.5723(0.5698) Encoder Grad: 0.1213  Decoder Grad: 0.2228  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17700/57574] Data 0.000 (0.000) Elapsed 219m 25s (remain 494m 16s) Loss: 0.6014(0.5696) Encoder Grad: 0.0615  Decoder Grad: 0.1960  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17800/57574] Data 0.000 (0.000) Elapsed 220m 42s (remain 493m 7s) Loss: 0.5221(0.5695) Encoder Grad: 0.2221  Decoder Grad: 0.2066  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][17900/57574] Data 0.000 (0.000) Elapsed 221m 58s (remain 491m 57s) Loss: 0.5357(0.5693) Encoder Grad: 0.1462  Decoder Grad: 0.1703  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][18000/57574] Data 0.000 (0.000) Elapsed 223m 16s (remain 490m 50s) Loss: 0.4971(0.5692) Encoder Grad: 1.3487  Decoder Grad: 0.4773  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][18100/57574] Data 0.000 (0.000) Elapsed 224m 32s (remain 489m 39s) Loss: 0.5046(0.5690) Encoder Grad: 0.1272  Decoder Grad: 0.1740  Encoder LR: 0.000100  Decoder LR: 0.000400  \n",
      "Epoch: [1][18200/57574] Data 0.000 (0.000) Elapsed 225m 47s (remain 488m 26s) Loss: 0.5273(0.5689) Encoder Grad: 0.1977  Decoder Grad: 0.1992  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18300/57574] Data 0.000 (0.000) Elapsed 227m 0s (remain 487m 9s) Loss: 0.6036(0.5688) Encoder Grad: 0.2688  Decoder Grad: 0.2360  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18400/57574] Data 0.000 (0.000) Elapsed 228m 14s (remain 485m 52s) Loss: 0.5319(0.5686) Encoder Grad: 0.0676  Decoder Grad: 0.1956  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18500/57574] Data 0.000 (0.000) Elapsed 229m 27s (remain 484m 35s) Loss: 0.6029(0.5684) Encoder Grad: 0.0378  Decoder Grad: 0.1583  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18600/57574] Data 0.000 (0.000) Elapsed 230m 38s (remain 483m 14s) Loss: 0.5030(0.5683) Encoder Grad: 0.7206  Decoder Grad: 0.3687  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18700/57574] Data 0.000 (0.000) Elapsed 231m 49s (remain 481m 52s) Loss: 0.5500(0.5680) Encoder Grad: 0.0395  Decoder Grad: 0.1815  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18800/57574] Data 0.000 (0.000) Elapsed 233m 0s (remain 480m 32s) Loss: 0.4495(0.5678) Encoder Grad: 0.4466  Decoder Grad: 0.2125  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][18900/57574] Data 0.000 (0.000) Elapsed 234m 8s (remain 479m 4s) Loss: 0.5854(0.5676) Encoder Grad: 0.0585  Decoder Grad: 0.2044  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19000/57574] Data 0.000 (0.000) Elapsed 235m 18s (remain 477m 41s) Loss: 0.5300(0.5674) Encoder Grad: 0.0753  Decoder Grad: 0.1645  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19100/57574] Data 0.000 (0.000) Elapsed 236m 27s (remain 476m 16s) Loss: 0.5217(0.5672) Encoder Grad: 0.2092  Decoder Grad: 0.3045  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19200/57574] Data 0.000 (0.000) Elapsed 237m 35s (remain 474m 48s) Loss: 0.4654(0.5670) Encoder Grad: 0.0844  Decoder Grad: 0.1789  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19300/57574] Data 0.000 (0.000) Elapsed 238m 50s (remain 473m 37s) Loss: 0.5056(0.5668) Encoder Grad: 0.6718  Decoder Grad: 0.3495  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19400/57574] Data 0.000 (0.000) Elapsed 240m 6s (remain 472m 26s) Loss: 0.4742(0.5666) Encoder Grad: 0.1080  Decoder Grad: 0.1941  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19500/57574] Data 0.000 (0.000) Elapsed 241m 18s (remain 471m 6s) Loss: 0.5090(0.5663) Encoder Grad: 0.0761  Decoder Grad: 0.1960  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19600/57574] Data 0.000 (0.000) Elapsed 242m 29s (remain 469m 46s) Loss: 0.5411(0.5660) Encoder Grad: 0.2162  Decoder Grad: 0.3548  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19700/57574] Data 0.000 (0.000) Elapsed 243m 44s (remain 468m 33s) Loss: 0.5139(0.5658) Encoder Grad: 0.1631  Decoder Grad: 0.2027  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19800/57574] Data 0.000 (0.000) Elapsed 244m 55s (remain 467m 14s) Loss: 0.5210(0.5656) Encoder Grad: 0.2978  Decoder Grad: 0.2589  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][19900/57574] Data 0.000 (0.000) Elapsed 246m 9s (remain 465m 58s) Loss: 0.5186(0.5654) Encoder Grad: 0.1057  Decoder Grad: 0.1655  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20000/57574] Data 0.000 (0.000) Elapsed 247m 23s (remain 464m 43s) Loss: 0.5000(0.5652) Encoder Grad: 0.0364  Decoder Grad: 0.1534  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20100/57574] Data 0.000 (0.000) Elapsed 248m 34s (remain 463m 23s) Loss: 0.5323(0.5649) Encoder Grad: 0.0893  Decoder Grad: 0.2441  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20200/57574] Data 0.000 (0.000) Elapsed 249m 49s (remain 462m 11s) Loss: 0.5766(0.5646) Encoder Grad: 0.0504  Decoder Grad: 0.1710  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20300/57574] Data 0.000 (0.000) Elapsed 251m 6s (remain 461m 2s) Loss: 0.4964(0.5644) Encoder Grad: 0.2187  Decoder Grad: 0.1878  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20400/57574] Data 0.000 (0.000) Elapsed 252m 22s (remain 459m 50s) Loss: 0.4619(0.5641) Encoder Grad: 0.0557  Decoder Grad: 0.1816  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20500/57574] Data 0.000 (0.000) Elapsed 253m 37s (remain 458m 38s) Loss: 0.4511(0.5638) Encoder Grad: 0.0400  Decoder Grad: 0.1833  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20600/57574] Data 0.000 (0.000) Elapsed 254m 49s (remain 457m 19s) Loss: 0.4707(0.5635) Encoder Grad: 0.1405  Decoder Grad: 0.2721  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20700/57574] Data 0.000 (0.000) Elapsed 256m 4s (remain 456m 8s) Loss: 0.5023(0.5632) Encoder Grad: 0.0312  Decoder Grad: 0.1968  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20800/57574] Data 0.000 (0.000) Elapsed 257m 21s (remain 454m 57s) Loss: 0.4816(0.5630) Encoder Grad: 0.0599  Decoder Grad: 0.1727  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][20900/57574] Data 0.000 (0.000) Elapsed 258m 36s (remain 453m 45s) Loss: 0.4941(0.5627) Encoder Grad: 0.0662  Decoder Grad: 0.1985  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21000/57574] Data 0.000 (0.000) Elapsed 259m 47s (remain 452m 25s) Loss: 0.4866(0.5624) Encoder Grad: 0.0276  Decoder Grad: 0.1601  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21100/57574] Data 0.000 (0.000) Elapsed 261m 2s (remain 451m 12s) Loss: 0.5306(0.5621) Encoder Grad: 1.1128  Decoder Grad: 0.4172  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21200/57574] Data 0.000 (0.000) Elapsed 262m 13s (remain 449m 52s) Loss: 0.4698(0.5618) Encoder Grad: 0.0497  Decoder Grad: 0.1811  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21300/57574] Data 0.000 (0.000) Elapsed 263m 24s (remain 448m 33s) Loss: 0.4794(0.5616) Encoder Grad: 0.0893  Decoder Grad: 0.1818  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21400/57574] Data 0.000 (0.000) Elapsed 264m 36s (remain 447m 15s) Loss: 0.5284(0.5613) Encoder Grad: 0.1889  Decoder Grad: 0.1949  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21500/57574] Data 0.000 (0.000) Elapsed 265m 51s (remain 446m 1s) Loss: 0.4712(0.5610) Encoder Grad: 0.0381  Decoder Grad: 0.1525  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21600/57574] Data 0.000 (0.000) Elapsed 267m 7s (remain 444m 51s) Loss: 0.5603(0.5608) Encoder Grad: 0.0788  Decoder Grad: 0.1971  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21700/57574] Data 0.000 (0.000) Elapsed 268m 18s (remain 443m 31s) Loss: 0.4869(0.5606) Encoder Grad: 0.0579  Decoder Grad: 0.2014  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21800/57574] Data 0.000 (0.000) Elapsed 269m 31s (remain 442m 14s) Loss: 0.5481(0.5604) Encoder Grad: 0.0513  Decoder Grad: 0.1730  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][21900/57574] Data 0.000 (0.000) Elapsed 270m 46s (remain 441m 3s) Loss: 0.5310(0.5602) Encoder Grad: 0.2287  Decoder Grad: 0.1985  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22000/57574] Data 0.000 (0.000) Elapsed 272m 2s (remain 439m 52s) Loss: 0.5050(0.5600) Encoder Grad: 0.1114  Decoder Grad: 0.2486  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22100/57574] Data 0.000 (0.000) Elapsed 273m 14s (remain 438m 33s) Loss: 0.5633(0.5598) Encoder Grad: 0.0801  Decoder Grad: 0.1730  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22200/57574] Data 0.000 (0.000) Elapsed 274m 30s (remain 437m 23s) Loss: 0.6410(0.5597) Encoder Grad: 0.3198  Decoder Grad: 0.3974  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22300/57574] Data 0.000 (0.000) Elapsed 275m 44s (remain 436m 8s) Loss: 0.5601(0.5595) Encoder Grad: 0.0585  Decoder Grad: 0.2003  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22400/57574] Data 0.000 (0.000) Elapsed 276m 57s (remain 434m 51s) Loss: 0.4942(0.5593) Encoder Grad: 0.0467  Decoder Grad: 0.1792  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22500/57574] Data 0.000 (0.000) Elapsed 278m 10s (remain 433m 35s) Loss: 0.5188(0.5591) Encoder Grad: 0.7057  Decoder Grad: 0.7102  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22600/57574] Data 0.000 (0.000) Elapsed 279m 23s (remain 432m 20s) Loss: 0.4833(0.5589) Encoder Grad: 0.1805  Decoder Grad: 0.1583  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22700/57574] Data 0.000 (0.000) Elapsed 280m 36s (remain 431m 3s) Loss: 0.4793(0.5587) Encoder Grad: 0.0657  Decoder Grad: 0.2984  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22800/57574] Data 0.000 (0.000) Elapsed 281m 50s (remain 429m 49s) Loss: 0.5503(0.5585) Encoder Grad: 0.0298  Decoder Grad: 0.1824  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][22900/57574] Data 0.000 (0.000) Elapsed 283m 1s (remain 428m 30s) Loss: 0.5437(0.5583) Encoder Grad: 0.0384  Decoder Grad: 0.1936  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23000/57574] Data 0.000 (0.000) Elapsed 284m 12s (remain 427m 12s) Loss: 0.5498(0.5581) Encoder Grad: 0.1080  Decoder Grad: 0.2025  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23100/57574] Data 0.000 (0.000) Elapsed 285m 24s (remain 425m 54s) Loss: 0.5247(0.5579) Encoder Grad: 0.1527  Decoder Grad: 0.3154  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23200/57574] Data 0.000 (0.000) Elapsed 286m 38s (remain 424m 40s) Loss: 0.5149(0.5577) Encoder Grad: 0.0604  Decoder Grad: 0.1796  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23300/57574] Data 0.000 (0.000) Elapsed 287m 52s (remain 423m 26s) Loss: 0.4926(0.5575) Encoder Grad: 0.0561  Decoder Grad: 0.1782  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23400/57574] Data 0.000 (0.000) Elapsed 289m 5s (remain 422m 9s) Loss: 0.5135(0.5573) Encoder Grad: 0.1017  Decoder Grad: 0.2135  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23500/57574] Data 0.000 (0.000) Elapsed 290m 17s (remain 420m 53s) Loss: 0.4388(0.5570) Encoder Grad: 0.0792  Decoder Grad: 0.2321  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23600/57574] Data 0.000 (0.000) Elapsed 291m 29s (remain 419m 35s) Loss: 0.5998(0.5568) Encoder Grad: 0.1335  Decoder Grad: 0.2268  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23700/57574] Data 0.000 (0.000) Elapsed 292m 45s (remain 418m 24s) Loss: 0.5241(0.5565) Encoder Grad: 0.0721  Decoder Grad: 0.1806  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23800/57574] Data 0.000 (0.000) Elapsed 293m 57s (remain 417m 7s) Loss: 0.5607(0.5563) Encoder Grad: 0.0615  Decoder Grad: 0.1876  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][23900/57574] Data 0.000 (0.000) Elapsed 295m 13s (remain 415m 55s) Loss: 0.5637(0.5561) Encoder Grad: 0.0578  Decoder Grad: 0.2276  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24000/57574] Data 0.000 (0.000) Elapsed 296m 24s (remain 414m 37s) Loss: 0.4938(0.5559) Encoder Grad: 0.0876  Decoder Grad: 0.1860  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24100/57574] Data 0.000 (0.000) Elapsed 297m 41s (remain 413m 27s) Loss: 0.6064(0.5557) Encoder Grad: 0.0988  Decoder Grad: 0.2334  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24200/57574] Data 0.000 (0.000) Elapsed 298m 53s (remain 412m 10s) Loss: 0.5664(0.5555) Encoder Grad: 0.0687  Decoder Grad: 0.1781  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24300/57574] Data 0.000 (0.000) Elapsed 300m 5s (remain 410m 52s) Loss: 0.4506(0.5553) Encoder Grad: 0.0769  Decoder Grad: 0.1857  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24400/57574] Data 0.000 (0.000) Elapsed 301m 23s (remain 409m 44s) Loss: 0.5630(0.5551) Encoder Grad: 0.0722  Decoder Grad: 0.1699  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24500/57574] Data 0.000 (0.000) Elapsed 302m 38s (remain 408m 31s) Loss: 0.4962(0.5549) Encoder Grad: 0.0320  Decoder Grad: 0.1449  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24600/57574] Data 0.000 (0.000) Elapsed 303m 49s (remain 407m 13s) Loss: 0.4989(0.5548) Encoder Grad: 0.0800  Decoder Grad: 0.2286  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24700/57574] Data 0.000 (0.000) Elapsed 305m 3s (remain 405m 58s) Loss: 0.5131(0.5546) Encoder Grad: 0.1009  Decoder Grad: 0.2271  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24800/57574] Data 0.000 (0.000) Elapsed 306m 17s (remain 404m 44s) Loss: 0.5346(0.5544) Encoder Grad: 0.1286  Decoder Grad: 0.2328  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][24900/57574] Data 0.000 (0.000) Elapsed 307m 29s (remain 403m 27s) Loss: 0.5579(0.5543) Encoder Grad: 0.6452  Decoder Grad: 0.2808  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25000/57574] Data 0.000 (0.000) Elapsed 308m 45s (remain 402m 16s) Loss: 0.6136(0.5541) Encoder Grad: 0.0349  Decoder Grad: 0.1796  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25100/57574] Data 0.000 (0.000) Elapsed 309m 59s (remain 401m 2s) Loss: 0.5068(0.5540) Encoder Grad: 0.0346  Decoder Grad: 0.2094  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25200/57574] Data 0.000 (0.000) Elapsed 311m 11s (remain 399m 44s) Loss: 0.5529(0.5539) Encoder Grad: 0.0393  Decoder Grad: 0.2549  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25300/57574] Data 0.000 (0.000) Elapsed 312m 29s (remain 398m 36s) Loss: 0.6010(0.5538) Encoder Grad: 0.0419  Decoder Grad: 0.1858  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25400/57574] Data 0.000 (0.000) Elapsed 313m 48s (remain 397m 27s) Loss: 0.5306(0.5537) Encoder Grad: 0.0544  Decoder Grad: 0.1634  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25500/57574] Data 0.000 (0.000) Elapsed 315m 6s (remain 396m 18s) Loss: 0.5308(0.5536) Encoder Grad: 0.0357  Decoder Grad: 0.1913  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25600/57574] Data 0.000 (0.000) Elapsed 316m 18s (remain 395m 1s) Loss: 0.5252(0.5534) Encoder Grad: 0.0570  Decoder Grad: 0.1828  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25700/57574] Data 0.000 (0.000) Elapsed 317m 29s (remain 393m 44s) Loss: 0.5273(0.5532) Encoder Grad: 0.0831  Decoder Grad: 0.2940  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25800/57574] Data 0.000 (0.000) Elapsed 318m 40s (remain 392m 26s) Loss: 0.5078(0.5531) Encoder Grad: 0.2178  Decoder Grad: 0.1979  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][25900/57574] Data 0.000 (0.000) Elapsed 319m 52s (remain 391m 9s) Loss: 0.4763(0.5529) Encoder Grad: 0.0660  Decoder Grad: 0.1837  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26000/57574] Data 0.000 (0.000) Elapsed 321m 3s (remain 389m 51s) Loss: 0.4816(0.5527) Encoder Grad: 0.0803  Decoder Grad: 0.2253  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26100/57574] Data 0.000 (0.000) Elapsed 322m 14s (remain 388m 34s) Loss: 0.5242(0.5526) Encoder Grad: 0.7334  Decoder Grad: 0.3065  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26200/57574] Data 0.000 (0.000) Elapsed 323m 25s (remain 387m 16s) Loss: 0.4432(0.5525) Encoder Grad: 0.1379  Decoder Grad: 0.1910  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26300/57574] Data 0.000 (0.000) Elapsed 324m 37s (remain 385m 59s) Loss: 0.4990(0.5523) Encoder Grad: 0.2116  Decoder Grad: 0.2247  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26400/57574] Data 0.000 (0.000) Elapsed 325m 49s (remain 384m 43s) Loss: 0.5477(0.5522) Encoder Grad: 0.0394  Decoder Grad: 0.1859  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26500/57574] Data 0.000 (0.000) Elapsed 327m 1s (remain 383m 26s) Loss: 0.4520(0.5521) Encoder Grad: 0.0560  Decoder Grad: 0.1730  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26600/57574] Data 0.000 (0.000) Elapsed 328m 12s (remain 382m 8s) Loss: 0.4674(0.5520) Encoder Grad: 0.2137  Decoder Grad: 0.2165  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26700/57574] Data 0.000 (0.000) Elapsed 329m 24s (remain 380m 52s) Loss: 0.5032(0.5518) Encoder Grad: 0.1641  Decoder Grad: 0.3520  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26800/57574] Data 0.000 (0.000) Elapsed 330m 35s (remain 379m 35s) Loss: 0.4486(0.5517) Encoder Grad: 0.0834  Decoder Grad: 0.2013  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][26900/57574] Data 0.000 (0.000) Elapsed 331m 47s (remain 378m 19s) Loss: 0.5118(0.5516) Encoder Grad: 0.0916  Decoder Grad: 0.1749  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27000/57574] Data 0.000 (0.000) Elapsed 332m 58s (remain 377m 1s) Loss: 0.5288(0.5514) Encoder Grad: 0.0421  Decoder Grad: 0.1814  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27100/57574] Data 0.000 (0.000) Elapsed 334m 9s (remain 375m 44s) Loss: 0.5250(0.5512) Encoder Grad: 0.1194  Decoder Grad: 0.2086  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27200/57574] Data 0.000 (0.000) Elapsed 335m 21s (remain 374m 27s) Loss: 0.5069(0.5511) Encoder Grad: 0.0418  Decoder Grad: 0.1898  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27300/57574] Data 0.000 (0.000) Elapsed 336m 33s (remain 373m 11s) Loss: 0.6044(0.5509) Encoder Grad: 0.7303  Decoder Grad: 0.4206  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27400/57574] Data 0.000 (0.000) Elapsed 337m 47s (remain 371m 57s) Loss: 0.5681(0.5508) Encoder Grad: 0.0321  Decoder Grad: 0.1865  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27500/57574] Data 0.000 (0.000) Elapsed 339m 2s (remain 370m 45s) Loss: 0.4775(0.5506) Encoder Grad: 0.3236  Decoder Grad: 0.2005  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27600/57574] Data 0.000 (0.000) Elapsed 340m 19s (remain 369m 33s) Loss: 0.4786(0.5505) Encoder Grad: 0.0727  Decoder Grad: 0.2363  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27700/57574] Data 0.000 (0.000) Elapsed 341m 34s (remain 368m 21s) Loss: 0.6475(0.5503) Encoder Grad: 0.0446  Decoder Grad: 0.1957  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27800/57574] Data 0.000 (0.000) Elapsed 342m 50s (remain 367m 9s) Loss: 0.5716(0.5502) Encoder Grad: 0.2707  Decoder Grad: 0.2365  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][27900/57574] Data 0.000 (0.000) Elapsed 344m 5s (remain 365m 56s) Loss: 0.6061(0.5500) Encoder Grad: 0.0837  Decoder Grad: 0.2313  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28000/57574] Data 0.000 (0.000) Elapsed 345m 21s (remain 364m 44s) Loss: 0.5906(0.5498) Encoder Grad: 0.1144  Decoder Grad: 0.2035  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28100/57574] Data 0.000 (0.000) Elapsed 346m 36s (remain 363m 31s) Loss: 0.4743(0.5497) Encoder Grad: 0.0684  Decoder Grad: 0.2201  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28200/57574] Data 0.000 (0.000) Elapsed 347m 51s (remain 362m 18s) Loss: 0.4808(0.5496) Encoder Grad: 0.0754  Decoder Grad: 0.2036  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28300/57574] Data 0.000 (0.000) Elapsed 349m 7s (remain 361m 6s) Loss: 0.5932(0.5495) Encoder Grad: 0.0943  Decoder Grad: 0.2125  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28400/57574] Data 0.000 (0.000) Elapsed 350m 21s (remain 359m 53s) Loss: 0.5376(0.5494) Encoder Grad: 0.1603  Decoder Grad: 0.1926  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28500/57574] Data 0.000 (0.000) Elapsed 351m 37s (remain 358m 41s) Loss: 0.5779(0.5492) Encoder Grad: 0.0581  Decoder Grad: 0.1896  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28600/57574] Data 0.000 (0.000) Elapsed 352m 53s (remain 357m 28s) Loss: 0.5701(0.5491) Encoder Grad: 0.2299  Decoder Grad: 0.2293  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28700/57574] Data 0.000 (0.000) Elapsed 354m 8s (remain 356m 16s) Loss: 0.5440(0.5490) Encoder Grad: 0.0839  Decoder Grad: 0.2146  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28800/57574] Data 0.000 (0.000) Elapsed 355m 24s (remain 355m 4s) Loss: 0.5480(0.5488) Encoder Grad: 0.0680  Decoder Grad: 0.2065  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][28900/57574] Data 0.000 (0.000) Elapsed 356m 40s (remain 353m 51s) Loss: 0.5193(0.5488) Encoder Grad: 0.3358  Decoder Grad: 0.2235  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29000/57574] Data 0.000 (0.000) Elapsed 357m 55s (remain 352m 38s) Loss: 0.5282(0.5486) Encoder Grad: 0.0674  Decoder Grad: 0.1665  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29100/57574] Data 0.000 (0.000) Elapsed 359m 11s (remain 351m 26s) Loss: 0.4679(0.5485) Encoder Grad: 0.1657  Decoder Grad: 0.1774  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29200/57574] Data 0.000 (0.000) Elapsed 360m 24s (remain 350m 11s) Loss: 0.5015(0.5483) Encoder Grad: 0.0762  Decoder Grad: 0.1740  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29300/57574] Data 0.000 (0.000) Elapsed 361m 34s (remain 348m 53s) Loss: 0.5183(0.5482) Encoder Grad: 0.2872  Decoder Grad: 0.3081  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29400/57574] Data 0.000 (0.000) Elapsed 362m 43s (remain 347m 34s) Loss: 0.4547(0.5481) Encoder Grad: 0.0457  Decoder Grad: 0.1834  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29500/57574] Data 0.000 (0.000) Elapsed 363m 53s (remain 346m 16s) Loss: 0.4836(0.5479) Encoder Grad: 0.0436  Decoder Grad: 0.1804  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29600/57574] Data 0.000 (0.000) Elapsed 365m 3s (remain 344m 58s) Loss: 0.5509(0.5478) Encoder Grad: 0.0787  Decoder Grad: 0.2942  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29700/57574] Data 0.000 (0.000) Elapsed 366m 13s (remain 343m 41s) Loss: 0.5627(0.5476) Encoder Grad: 0.0326  Decoder Grad: 0.1755  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29800/57574] Data 0.000 (0.000) Elapsed 367m 22s (remain 342m 22s) Loss: 0.4815(0.5475) Encoder Grad: 0.1838  Decoder Grad: 0.2666  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][29900/57574] Data 0.000 (0.000) Elapsed 368m 32s (remain 341m 4s) Loss: 0.5042(0.5473) Encoder Grad: 0.0605  Decoder Grad: 0.1800  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30000/57574] Data 0.000 (0.000) Elapsed 369m 42s (remain 339m 46s) Loss: 0.5146(0.5472) Encoder Grad: 0.0637  Decoder Grad: 0.2125  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30100/57574] Data 0.000 (0.000) Elapsed 370m 51s (remain 338m 28s) Loss: 0.5298(0.5470) Encoder Grad: 0.0611  Decoder Grad: 0.2259  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30200/57574] Data 0.000 (0.000) Elapsed 372m 1s (remain 337m 10s) Loss: 0.4920(0.5469) Encoder Grad: 0.0513  Decoder Grad: 0.2076  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30300/57574] Data 0.000 (0.000) Elapsed 373m 10s (remain 335m 53s) Loss: 0.4536(0.5468) Encoder Grad: 0.2245  Decoder Grad: 0.2407  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30400/57574] Data 0.000 (0.000) Elapsed 374m 20s (remain 334m 35s) Loss: 0.5188(0.5467) Encoder Grad: 0.1092  Decoder Grad: 0.1832  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30500/57574] Data 0.000 (0.000) Elapsed 375m 30s (remain 333m 18s) Loss: 0.5208(0.5465) Encoder Grad: 0.0262  Decoder Grad: 0.1629  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30600/57574] Data 0.000 (0.000) Elapsed 376m 39s (remain 332m 0s) Loss: 0.4756(0.5464) Encoder Grad: 0.1540  Decoder Grad: 0.2794  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30700/57574] Data 0.000 (0.000) Elapsed 377m 49s (remain 330m 42s) Loss: 0.4564(0.5462) Encoder Grad: 0.0828  Decoder Grad: 0.2277  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30800/57574] Data 0.000 (0.000) Elapsed 378m 59s (remain 329m 25s) Loss: 0.5826(0.5461) Encoder Grad: 0.1023  Decoder Grad: 0.2143  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][30900/57574] Data 0.000 (0.000) Elapsed 380m 7s (remain 328m 7s) Loss: 0.5023(0.5459) Encoder Grad: 0.0288  Decoder Grad: 0.1703  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31000/57574] Data 0.000 (0.000) Elapsed 381m 17s (remain 326m 49s) Loss: 0.4705(0.5458) Encoder Grad: 0.1068  Decoder Grad: 0.1944  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31100/57574] Data 0.000 (0.000) Elapsed 382m 27s (remain 325m 32s) Loss: 0.4960(0.5456) Encoder Grad: 0.1143  Decoder Grad: 0.1882  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31200/57574] Data 0.000 (0.000) Elapsed 383m 36s (remain 324m 14s) Loss: 0.4975(0.5454) Encoder Grad: 0.1669  Decoder Grad: 0.3300  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31300/57574] Data 0.000 (0.000) Elapsed 384m 44s (remain 322m 56s) Loss: 0.4987(0.5452) Encoder Grad: 0.0805  Decoder Grad: 0.1975  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31400/57574] Data 0.000 (0.000) Elapsed 385m 51s (remain 321m 37s) Loss: 0.4999(0.5451) Encoder Grad: 0.3392  Decoder Grad: 0.2429  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31500/57574] Data 0.000 (0.000) Elapsed 386m 58s (remain 320m 17s) Loss: 0.4711(0.5449) Encoder Grad: 0.0316  Decoder Grad: 0.1922  Encoder LR: 0.000099  Decoder LR: 0.000400  \n",
      "Epoch: [1][31600/57574] Data 0.000 (0.000) Elapsed 388m 4s (remain 318m 57s) Loss: 0.5049(0.5447) Encoder Grad: 0.2232  Decoder Grad: 0.2291  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][31700/57574] Data 0.000 (0.000) Elapsed 389m 13s (remain 317m 40s) Loss: 0.5002(0.5446) Encoder Grad: 0.0223  Decoder Grad: 0.1777  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][31800/57574] Data 0.000 (0.000) Elapsed 390m 22s (remain 316m 22s) Loss: 0.5426(0.5444) Encoder Grad: 0.0500  Decoder Grad: 0.1919  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][31900/57574] Data 0.000 (0.000) Elapsed 391m 31s (remain 315m 5s) Loss: 0.5087(0.5442) Encoder Grad: 0.2269  Decoder Grad: 0.1929  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32000/57574] Data 0.000 (0.000) Elapsed 392m 41s (remain 313m 49s) Loss: 0.4445(0.5440) Encoder Grad: 0.0696  Decoder Grad: 0.1856  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32100/57574] Data 0.000 (0.000) Elapsed 393m 51s (remain 312m 32s) Loss: 0.4630(0.5439) Encoder Grad: 0.1810  Decoder Grad: 0.1800  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32200/57574] Data 0.000 (0.000) Elapsed 395m 2s (remain 311m 16s) Loss: 0.4801(0.5437) Encoder Grad: 0.2635  Decoder Grad: 0.2975  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32300/57574] Data 0.000 (0.000) Elapsed 396m 12s (remain 310m 0s) Loss: 0.5094(0.5435) Encoder Grad: 0.1248  Decoder Grad: 0.3136  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32400/57574] Data 0.000 (0.000) Elapsed 397m 21s (remain 308m 42s) Loss: 0.5077(0.5433) Encoder Grad: 0.0567  Decoder Grad: 0.2389  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32500/57574] Data 0.000 (0.000) Elapsed 398m 28s (remain 307m 24s) Loss: 0.4628(0.5432) Encoder Grad: 0.0381  Decoder Grad: 0.2140  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32600/57574] Data 0.000 (0.000) Elapsed 399m 37s (remain 306m 6s) Loss: 0.4769(0.5430) Encoder Grad: 0.7115  Decoder Grad: 0.2401  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32700/57574] Data 0.000 (0.000) Elapsed 400m 45s (remain 304m 49s) Loss: 0.4669(0.5429) Encoder Grad: 0.0521  Decoder Grad: 0.2040  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32800/57574] Data 0.000 (0.000) Elapsed 401m 53s (remain 303m 31s) Loss: 0.5391(0.5428) Encoder Grad: 0.1611  Decoder Grad: 0.3099  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][32900/57574] Data 0.000 (0.000) Elapsed 403m 1s (remain 302m 13s) Loss: 0.5603(0.5426) Encoder Grad: 0.0705  Decoder Grad: 0.1828  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33000/57574] Data 0.000 (0.000) Elapsed 404m 8s (remain 300m 55s) Loss: 0.5605(0.5425) Encoder Grad: 0.1292  Decoder Grad: 0.1706  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33100/57574] Data 0.000 (0.000) Elapsed 405m 15s (remain 299m 37s) Loss: 0.4622(0.5424) Encoder Grad: 0.0370  Decoder Grad: 0.1920  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33200/57574] Data 0.000 (0.000) Elapsed 406m 21s (remain 298m 18s) Loss: 0.4889(0.5423) Encoder Grad: 0.0196  Decoder Grad: 0.1754  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33300/57574] Data 0.000 (0.000) Elapsed 407m 28s (remain 297m 0s) Loss: 0.4842(0.5422) Encoder Grad: 0.4290  Decoder Grad: 0.2447  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33400/57574] Data 0.000 (0.000) Elapsed 408m 35s (remain 295m 42s) Loss: 0.5115(0.5420) Encoder Grad: 0.0453  Decoder Grad: 0.1672  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33500/57574] Data 0.000 (0.000) Elapsed 409m 41s (remain 294m 23s) Loss: 0.5116(0.5419) Encoder Grad: 0.0857  Decoder Grad: 0.2241  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33600/57574] Data 0.000 (0.000) Elapsed 410m 47s (remain 293m 5s) Loss: 0.5664(0.5418) Encoder Grad: 0.0574  Decoder Grad: 0.2082  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33700/57574] Data 0.000 (0.000) Elapsed 411m 53s (remain 291m 46s) Loss: 0.5628(0.5417) Encoder Grad: 0.0767  Decoder Grad: 0.2431  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33800/57574] Data 0.000 (0.000) Elapsed 412m 59s (remain 290m 28s) Loss: 0.4809(0.5416) Encoder Grad: 0.0464  Decoder Grad: 0.2008  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][33900/57574] Data 0.000 (0.000) Elapsed 414m 5s (remain 289m 9s) Loss: 0.4941(0.5415) Encoder Grad: 0.0671  Decoder Grad: 0.2002  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34000/57574] Data 0.000 (0.000) Elapsed 415m 12s (remain 287m 51s) Loss: 0.4526(0.5414) Encoder Grad: 0.0794  Decoder Grad: 0.2349  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34100/57574] Data 0.000 (0.000) Elapsed 416m 18s (remain 286m 33s) Loss: 0.4521(0.5413) Encoder Grad: 0.1033  Decoder Grad: 0.3415  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34200/57574] Data 0.000 (0.000) Elapsed 417m 24s (remain 285m 15s) Loss: 0.5112(0.5411) Encoder Grad: 0.3037  Decoder Grad: 0.7473  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34300/57574] Data 0.000 (0.000) Elapsed 418m 30s (remain 283m 57s) Loss: 0.4630(0.5410) Encoder Grad: 0.2396  Decoder Grad: 0.3234  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34400/57574] Data 0.000 (0.000) Elapsed 419m 36s (remain 282m 39s) Loss: 0.4710(0.5408) Encoder Grad: 0.1431  Decoder Grad: 0.2331  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34500/57574] Data 0.000 (0.000) Elapsed 420m 43s (remain 281m 21s) Loss: 0.5197(0.5407) Encoder Grad: 0.1927  Decoder Grad: 0.2257  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34600/57574] Data 0.000 (0.000) Elapsed 421m 48s (remain 280m 3s) Loss: 0.4943(0.5406) Encoder Grad: 0.0759  Decoder Grad: 0.1957  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34700/57574] Data 0.000 (0.000) Elapsed 422m 54s (remain 278m 45s) Loss: 0.4959(0.5404) Encoder Grad: 0.0419  Decoder Grad: 0.1928  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34800/57574] Data 0.000 (0.000) Elapsed 424m 1s (remain 277m 28s) Loss: 0.4758(0.5402) Encoder Grad: 0.0360  Decoder Grad: 0.1942  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][34900/57574] Data 0.000 (0.000) Elapsed 425m 7s (remain 276m 10s) Loss: 0.4230(0.5401) Encoder Grad: 4.6083  Decoder Grad: 1.2320  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35000/57574] Data 0.000 (0.000) Elapsed 426m 14s (remain 274m 53s) Loss: 0.4015(0.5399) Encoder Grad: 0.0407  Decoder Grad: 0.1773  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35100/57574] Data 0.000 (0.000) Elapsed 427m 20s (remain 273m 36s) Loss: 0.4331(0.5398) Encoder Grad: 0.1281  Decoder Grad: 0.2172  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35200/57574] Data 0.000 (0.000) Elapsed 428m 27s (remain 272m 18s) Loss: 0.4909(0.5396) Encoder Grad: 0.4263  Decoder Grad: 0.6026  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35300/57574] Data 0.000 (0.000) Elapsed 429m 32s (remain 271m 1s) Loss: 0.4849(0.5394) Encoder Grad: 0.1490  Decoder Grad: 0.2486  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35400/57574] Data 0.000 (0.000) Elapsed 430m 38s (remain 269m 43s) Loss: 0.5052(0.5393) Encoder Grad: 0.2611  Decoder Grad: 0.2106  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35500/57574] Data 0.000 (0.000) Elapsed 431m 44s (remain 268m 26s) Loss: 0.4304(0.5391) Encoder Grad: 0.1309  Decoder Grad: 0.2608  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35600/57574] Data 0.000 (0.000) Elapsed 432m 50s (remain 267m 9s) Loss: 0.4431(0.5389) Encoder Grad: 0.4907  Decoder Grad: 0.2557  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35700/57574] Data 0.000 (0.000) Elapsed 433m 57s (remain 265m 52s) Loss: 0.5045(0.5388) Encoder Grad: 0.0500  Decoder Grad: 0.2063  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35800/57574] Data 0.000 (0.000) Elapsed 435m 3s (remain 264m 35s) Loss: 0.4532(0.5386) Encoder Grad: 0.1082  Decoder Grad: 0.1910  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][35900/57574] Data 0.000 (0.000) Elapsed 436m 9s (remain 263m 18s) Loss: 0.4762(0.5385) Encoder Grad: 0.3900  Decoder Grad: 0.2975  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36000/57574] Data 0.000 (0.000) Elapsed 437m 15s (remain 262m 1s) Loss: 0.4753(0.5383) Encoder Grad: 0.1385  Decoder Grad: 0.2785  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36100/57574] Data 0.000 (0.000) Elapsed 438m 21s (remain 260m 44s) Loss: 0.4723(0.5382) Encoder Grad: 0.0413  Decoder Grad: 0.2001  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36200/57574] Data 0.000 (0.000) Elapsed 439m 30s (remain 259m 29s) Loss: 0.5217(0.5381) Encoder Grad: 0.1606  Decoder Grad: 0.2805  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36300/57574] Data 0.000 (0.000) Elapsed 440m 40s (remain 258m 14s) Loss: 0.5045(0.5379) Encoder Grad: 0.1388  Decoder Grad: 0.2278  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36400/57574] Data 0.000 (0.000) Elapsed 441m 50s (remain 256m 59s) Loss: 0.4624(0.5378) Encoder Grad: 0.0580  Decoder Grad: 0.1920  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36500/57574] Data 0.000 (0.000) Elapsed 443m 0s (remain 255m 45s) Loss: 0.4812(0.5376) Encoder Grad: 0.1077  Decoder Grad: 0.2674  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36600/57574] Data 0.000 (0.000) Elapsed 444m 9s (remain 254m 30s) Loss: 0.4399(0.5375) Encoder Grad: 0.2788  Decoder Grad: 0.2402  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36700/57574] Data 0.000 (0.000) Elapsed 445m 18s (remain 253m 15s) Loss: 0.5413(0.5373) Encoder Grad: 0.0509  Decoder Grad: 0.1883  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36800/57574] Data 0.000 (0.000) Elapsed 446m 27s (remain 252m 0s) Loss: 0.4194(0.5371) Encoder Grad: 0.0281  Decoder Grad: 0.1644  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][36900/57574] Data 0.000 (0.000) Elapsed 447m 37s (remain 250m 46s) Loss: 0.5218(0.5369) Encoder Grad: 0.0506  Decoder Grad: 0.2169  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37000/57574] Data 0.000 (0.000) Elapsed 448m 46s (remain 249m 31s) Loss: 0.4829(0.5368) Encoder Grad: 0.1848  Decoder Grad: 0.2296  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37100/57574] Data 0.000 (0.000) Elapsed 449m 55s (remain 248m 16s) Loss: 0.4906(0.5366) Encoder Grad: 0.0327  Decoder Grad: 0.2217  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37200/57574] Data 0.000 (0.000) Elapsed 451m 6s (remain 247m 2s) Loss: 0.4532(0.5365) Encoder Grad: 0.0806  Decoder Grad: 0.2236  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37300/57574] Data 0.000 (0.000) Elapsed 452m 18s (remain 245m 49s) Loss: 0.5151(0.5364) Encoder Grad: 0.0312  Decoder Grad: 0.2171  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37400/57574] Data 0.000 (0.000) Elapsed 453m 30s (remain 244m 36s) Loss: 0.5146(0.5362) Encoder Grad: 0.1481  Decoder Grad: 0.2012  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37500/57574] Data 0.000 (0.000) Elapsed 454m 44s (remain 243m 24s) Loss: 0.5376(0.5361) Encoder Grad: 0.1045  Decoder Grad: 0.1928  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37600/57574] Data 0.000 (0.000) Elapsed 455m 57s (remain 242m 11s) Loss: 0.4840(0.5360) Encoder Grad: 0.1159  Decoder Grad: 0.2393  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37700/57574] Data 0.000 (0.000) Elapsed 457m 9s (remain 240m 58s) Loss: 0.5034(0.5358) Encoder Grad: 0.1262  Decoder Grad: 0.3608  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37800/57574] Data 0.000 (0.000) Elapsed 458m 22s (remain 239m 45s) Loss: 0.5072(0.5357) Encoder Grad: 0.0884  Decoder Grad: 0.2208  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][37900/57574] Data 0.000 (0.000) Elapsed 459m 35s (remain 238m 33s) Loss: 0.4419(0.5356) Encoder Grad: 0.1067  Decoder Grad: 0.1916  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38000/57574] Data 0.000 (0.000) Elapsed 460m 48s (remain 237m 20s) Loss: 0.4562(0.5354) Encoder Grad: 0.1916  Decoder Grad: 0.2213  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38100/57574] Data 0.000 (0.000) Elapsed 462m 0s (remain 236m 7s) Loss: 0.4710(0.5353) Encoder Grad: 0.0527  Decoder Grad: 0.2121  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38200/57574] Data 0.000 (0.000) Elapsed 463m 14s (remain 234m 55s) Loss: 0.4919(0.5352) Encoder Grad: 0.1511  Decoder Grad: 0.2330  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38300/57574] Data 0.000 (0.000) Elapsed 464m 26s (remain 233m 42s) Loss: 0.5089(0.5351) Encoder Grad: 0.1438  Decoder Grad: 0.2424  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38400/57574] Data 0.000 (0.000) Elapsed 465m 38s (remain 232m 29s) Loss: 0.5575(0.5349) Encoder Grad: 0.2269  Decoder Grad: 0.2246  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38500/57574] Data 0.000 (0.000) Elapsed 466m 51s (remain 231m 16s) Loss: 0.4943(0.5348) Encoder Grad: 0.2362  Decoder Grad: 0.2949  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38600/57574] Data 0.000 (0.000) Elapsed 468m 4s (remain 230m 3s) Loss: 0.4865(0.5347) Encoder Grad: 0.2388  Decoder Grad: 0.1985  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38700/57574] Data 0.000 (0.000) Elapsed 469m 17s (remain 228m 51s) Loss: 0.4400(0.5346) Encoder Grad: 0.1547  Decoder Grad: 0.2449  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38800/57574] Data 0.000 (0.000) Elapsed 470m 29s (remain 227m 38s) Loss: 0.4788(0.5344) Encoder Grad: 0.3602  Decoder Grad: 0.2148  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][38900/57574] Data 0.000 (0.000) Elapsed 471m 41s (remain 226m 24s) Loss: 0.5161(0.5343) Encoder Grad: 0.0700  Decoder Grad: 0.1938  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39000/57574] Data 0.000 (0.000) Elapsed 472m 50s (remain 225m 10s) Loss: 0.4859(0.5341) Encoder Grad: 0.0650  Decoder Grad: 0.2462  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39100/57574] Data 0.000 (0.000) Elapsed 474m 1s (remain 223m 56s) Loss: 0.4469(0.5340) Encoder Grad: 0.1883  Decoder Grad: 0.2978  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39200/57574] Data 0.000 (0.000) Elapsed 475m 11s (remain 222m 42s) Loss: 0.5267(0.5338) Encoder Grad: 0.1578  Decoder Grad: 0.2986  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39300/57574] Data 0.000 (0.000) Elapsed 476m 20s (remain 221m 28s) Loss: 0.4950(0.5337) Encoder Grad: 0.0315  Decoder Grad: 0.1840  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39400/57574] Data 0.000 (0.000) Elapsed 477m 30s (remain 220m 14s) Loss: 0.4706(0.5335) Encoder Grad: 0.1484  Decoder Grad: 0.2344  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39500/57574] Data 0.000 (0.000) Elapsed 478m 41s (remain 219m 0s) Loss: 0.4871(0.5334) Encoder Grad: 0.1757  Decoder Grad: 0.2369  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39600/57574] Data 0.000 (0.000) Elapsed 479m 51s (remain 217m 47s) Loss: 0.5015(0.5332) Encoder Grad: 0.1850  Decoder Grad: 0.3285  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39700/57574] Data 0.000 (0.000) Elapsed 481m 1s (remain 216m 33s) Loss: 0.4264(0.5331) Encoder Grad: 1.1490  Decoder Grad: 0.3478  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39800/57574] Data 0.000 (0.000) Elapsed 482m 11s (remain 215m 19s) Loss: 0.4697(0.5329) Encoder Grad: 0.0837  Decoder Grad: 0.1815  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][39900/57574] Data 0.000 (0.000) Elapsed 483m 21s (remain 214m 5s) Loss: 0.4846(0.5327) Encoder Grad: 0.2168  Decoder Grad: 0.2247  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40000/57574] Data 0.000 (0.000) Elapsed 484m 31s (remain 212m 51s) Loss: 0.4496(0.5326) Encoder Grad: 0.1348  Decoder Grad: 0.2501  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40100/57574] Data 0.000 (0.000) Elapsed 485m 41s (remain 211m 37s) Loss: 0.4393(0.5324) Encoder Grad: 0.0722  Decoder Grad: 0.2077  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40200/57574] Data 0.000 (0.000) Elapsed 486m 52s (remain 210m 24s) Loss: 0.4143(0.5323) Encoder Grad: 0.2140  Decoder Grad: 0.2668  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40300/57574] Data 0.000 (0.000) Elapsed 488m 1s (remain 209m 10s) Loss: 0.5150(0.5321) Encoder Grad: 0.0770  Decoder Grad: 0.2219  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40400/57574] Data 0.000 (0.000) Elapsed 489m 11s (remain 207m 56s) Loss: 0.5132(0.5320) Encoder Grad: 0.1008  Decoder Grad: 0.2101  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40500/57574] Data 0.000 (0.000) Elapsed 490m 21s (remain 206m 42s) Loss: 0.4586(0.5319) Encoder Grad: 0.1484  Decoder Grad: 0.3060  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40600/57574] Data 0.000 (0.000) Elapsed 491m 31s (remain 205m 28s) Loss: 0.4511(0.5317) Encoder Grad: 0.1651  Decoder Grad: 0.2122  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40700/57574] Data 0.000 (0.000) Elapsed 492m 42s (remain 204m 15s) Loss: 0.5031(0.5316) Encoder Grad: 0.2784  Decoder Grad: 0.2517  Encoder LR: 0.000098  Decoder LR: 0.000400  \n",
      "Epoch: [1][40800/57574] Data 0.000 (0.000) Elapsed 493m 51s (remain 203m 1s) Loss: 0.4373(0.5315) Encoder Grad: 0.1413  Decoder Grad: 0.2133  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][40900/57574] Data 0.000 (0.000) Elapsed 495m 1s (remain 201m 47s) Loss: 0.4839(0.5313) Encoder Grad: 0.2356  Decoder Grad: 0.2110  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41000/57574] Data 0.000 (0.000) Elapsed 496m 11s (remain 200m 33s) Loss: 0.4603(0.5312) Encoder Grad: 0.1867  Decoder Grad: 0.2039  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41100/57574] Data 0.000 (0.000) Elapsed 497m 21s (remain 199m 20s) Loss: 0.5170(0.5310) Encoder Grad: 0.0721  Decoder Grad: 0.2210  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41200/57574] Data 0.000 (0.000) Elapsed 498m 31s (remain 198m 6s) Loss: 0.4497(0.5309) Encoder Grad: 0.1833  Decoder Grad: 0.2613  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41300/57574] Data 0.000 (0.000) Elapsed 499m 41s (remain 196m 52s) Loss: 0.5099(0.5307) Encoder Grad: 0.2982  Decoder Grad: 0.5354  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41400/57574] Data 0.000 (0.000) Elapsed 500m 51s (remain 195m 39s) Loss: 0.4600(0.5306) Encoder Grad: 0.1697  Decoder Grad: 0.2151  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41500/57574] Data 0.000 (0.000) Elapsed 502m 1s (remain 194m 25s) Loss: 0.5380(0.5305) Encoder Grad: 0.1987  Decoder Grad: 0.2543  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41600/57574] Data 0.000 (0.000) Elapsed 503m 11s (remain 193m 12s) Loss: 0.4369(0.5303) Encoder Grad: 0.1258  Decoder Grad: 0.1889  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41700/57574] Data 0.000 (0.000) Elapsed 504m 21s (remain 191m 58s) Loss: 0.4236(0.5301) Encoder Grad: 0.0695  Decoder Grad: 0.2430  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41800/57574] Data 0.000 (0.000) Elapsed 505m 30s (remain 190m 44s) Loss: 0.4533(0.5299) Encoder Grad: 0.1322  Decoder Grad: 0.2448  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][41900/57574] Data 0.000 (0.000) Elapsed 506m 39s (remain 189m 31s) Loss: 0.4535(0.5298) Encoder Grad: 0.1460  Decoder Grad: 0.2428  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42000/57574] Data 0.000 (0.000) Elapsed 507m 49s (remain 188m 17s) Loss: 0.4619(0.5296) Encoder Grad: 0.0648  Decoder Grad: 0.1897  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42100/57574] Data 0.000 (0.000) Elapsed 509m 0s (remain 187m 4s) Loss: 0.5021(0.5295) Encoder Grad: 0.0886  Decoder Grad: 0.2222  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42200/57574] Data 0.000 (0.000) Elapsed 510m 10s (remain 185m 50s) Loss: 0.4650(0.5293) Encoder Grad: 0.1429  Decoder Grad: 0.2409  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42300/57574] Data 0.000 (0.000) Elapsed 511m 20s (remain 184m 37s) Loss: 0.4994(0.5292) Encoder Grad: 0.2267  Decoder Grad: 0.3427  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42400/57574] Data 0.000 (0.000) Elapsed 512m 30s (remain 183m 23s) Loss: 0.4423(0.5290) Encoder Grad: 0.0873  Decoder Grad: 0.2040  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42500/57574] Data 0.000 (0.000) Elapsed 513m 40s (remain 182m 10s) Loss: 0.4860(0.5288) Encoder Grad: 0.0924  Decoder Grad: 0.2742  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42600/57574] Data 0.000 (0.000) Elapsed 514m 49s (remain 180m 56s) Loss: 0.4704(0.5287) Encoder Grad: 0.0752  Decoder Grad: 0.2161  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42700/57574] Data 0.000 (0.000) Elapsed 516m 0s (remain 179m 43s) Loss: 0.4592(0.5286) Encoder Grad: 0.1136  Decoder Grad: 0.2558  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42800/57574] Data 0.000 (0.000) Elapsed 517m 10s (remain 178m 30s) Loss: 0.4469(0.5284) Encoder Grad: 0.0578  Decoder Grad: 0.2197  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][42900/57574] Data 0.000 (0.000) Elapsed 518m 20s (remain 177m 16s) Loss: 0.4826(0.5283) Encoder Grad: 0.0561  Decoder Grad: 0.2091  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43000/57574] Data 0.000 (0.000) Elapsed 519m 29s (remain 176m 3s) Loss: 0.5106(0.5282) Encoder Grad: 0.1919  Decoder Grad: 0.2608  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43100/57574] Data 0.000 (0.000) Elapsed 520m 39s (remain 174m 50s) Loss: 0.5486(0.5281) Encoder Grad: 0.0762  Decoder Grad: 0.2100  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43200/57574] Data 0.000 (0.000) Elapsed 521m 49s (remain 173m 36s) Loss: 0.4227(0.5280) Encoder Grad: 0.0875  Decoder Grad: 0.2102  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43300/57574] Data 0.000 (0.000) Elapsed 522m 59s (remain 172m 23s) Loss: 0.4947(0.5279) Encoder Grad: 0.9138  Decoder Grad: 0.5131  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43400/57574] Data 0.000 (0.000) Elapsed 524m 9s (remain 171m 10s) Loss: 0.4750(0.5278) Encoder Grad: 0.0733  Decoder Grad: 0.1919  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43500/57574] Data 0.000 (0.000) Elapsed 525m 19s (remain 169m 56s) Loss: 0.4161(0.5277) Encoder Grad: 0.0568  Decoder Grad: 0.2262  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43600/57574] Data 0.000 (0.000) Elapsed 526m 32s (remain 168m 44s) Loss: 0.4244(0.5275) Encoder Grad: 0.5496  Decoder Grad: 0.2870  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43700/57574] Data 0.000 (0.000) Elapsed 527m 45s (remain 167m 32s) Loss: 0.5226(0.5274) Encoder Grad: 0.1120  Decoder Grad: 0.2171  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43800/57574] Data 0.000 (0.000) Elapsed 528m 59s (remain 166m 20s) Loss: 0.4377(0.5273) Encoder Grad: 0.0636  Decoder Grad: 0.1902  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][43900/57574] Data 0.000 (0.000) Elapsed 530m 11s (remain 165m 7s) Loss: 0.4540(0.5272) Encoder Grad: 0.0689  Decoder Grad: 0.2156  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44000/57574] Data 0.000 (0.000) Elapsed 531m 24s (remain 163m 55s) Loss: 0.5150(0.5270) Encoder Grad: 0.1911  Decoder Grad: 0.2596  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44100/57574] Data 0.000 (0.000) Elapsed 532m 38s (remain 162m 43s) Loss: 0.3977(0.5269) Encoder Grad: 0.7465  Decoder Grad: 0.2408  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44200/57574] Data 0.000 (0.000) Elapsed 533m 51s (remain 161m 31s) Loss: 0.4621(0.5268) Encoder Grad: 0.1201  Decoder Grad: 0.3132  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44300/57574] Data 0.000 (0.000) Elapsed 535m 4s (remain 160m 18s) Loss: 0.4491(0.5266) Encoder Grad: 0.0886  Decoder Grad: 0.2299  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44400/57574] Data 0.000 (0.000) Elapsed 536m 16s (remain 159m 6s) Loss: 0.4773(0.5265) Encoder Grad: 0.2170  Decoder Grad: 0.2830  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44500/57574] Data 0.000 (0.000) Elapsed 537m 30s (remain 157m 54s) Loss: 0.5300(0.5264) Encoder Grad: 0.0900  Decoder Grad: 0.2346  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44600/57574] Data 0.000 (0.000) Elapsed 538m 43s (remain 156m 41s) Loss: 0.4827(0.5263) Encoder Grad: 0.1462  Decoder Grad: 0.2047  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44700/57574] Data 0.000 (0.000) Elapsed 539m 57s (remain 155m 29s) Loss: 0.4450(0.5261) Encoder Grad: 0.0729  Decoder Grad: 0.1979  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44800/57574] Data 0.000 (0.000) Elapsed 541m 9s (remain 154m 17s) Loss: 0.4322(0.5260) Encoder Grad: 0.0834  Decoder Grad: 0.2231  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][44900/57574] Data 0.000 (0.000) Elapsed 542m 21s (remain 153m 4s) Loss: 0.4680(0.5258) Encoder Grad: 0.1219  Decoder Grad: 0.2378  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45000/57574] Data 0.000 (0.000) Elapsed 543m 34s (remain 151m 52s) Loss: 0.4042(0.5257) Encoder Grad: 0.0474  Decoder Grad: 0.1980  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45100/57574] Data 0.000 (0.000) Elapsed 544m 45s (remain 150m 39s) Loss: 0.5072(0.5256) Encoder Grad: 0.1518  Decoder Grad: 0.3331  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45200/57574] Data 0.000 (0.000) Elapsed 545m 55s (remain 149m 26s) Loss: 0.4126(0.5254) Encoder Grad: 0.0480  Decoder Grad: 0.2207  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45300/57574] Data 0.000 (0.000) Elapsed 547m 4s (remain 148m 12s) Loss: 0.4256(0.5253) Encoder Grad: 0.1983  Decoder Grad: 0.2494  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45400/57574] Data 0.000 (0.000) Elapsed 548m 14s (remain 146m 59s) Loss: 0.3985(0.5252) Encoder Grad: 0.0471  Decoder Grad: 0.1970  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45500/57574] Data 0.000 (0.000) Elapsed 549m 22s (remain 145m 46s) Loss: 0.4909(0.5250) Encoder Grad: 0.6687  Decoder Grad: 0.9133  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45600/57574] Data 0.000 (0.000) Elapsed 550m 31s (remain 144m 32s) Loss: 0.4636(0.5248) Encoder Grad: 0.4062  Decoder Grad: 0.3059  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45700/57574] Data 0.000 (0.000) Elapsed 551m 42s (remain 143m 19s) Loss: 0.4427(0.5247) Encoder Grad: 0.0853  Decoder Grad: 0.2261  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45800/57574] Data 0.000 (0.000) Elapsed 552m 52s (remain 142m 6s) Loss: 0.4820(0.5246) Encoder Grad: 0.1356  Decoder Grad: 0.2271  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][45900/57574] Data 0.000 (0.000) Elapsed 554m 1s (remain 140m 53s) Loss: 0.4237(0.5244) Encoder Grad: 0.2700  Decoder Grad: 0.3296  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46000/57574] Data 0.000 (0.000) Elapsed 555m 10s (remain 139m 40s) Loss: 0.4246(0.5243) Encoder Grad: 0.1277  Decoder Grad: 0.2807  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46100/57574] Data 0.000 (0.000) Elapsed 556m 20s (remain 138m 27s) Loss: 0.4997(0.5242) Encoder Grad: 0.0531  Decoder Grad: 0.2383  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46200/57574] Data 0.000 (0.000) Elapsed 557m 30s (remain 137m 14s) Loss: 0.4864(0.5241) Encoder Grad: 0.0716  Decoder Grad: 0.2385  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46300/57574] Data 0.000 (0.000) Elapsed 558m 39s (remain 136m 1s) Loss: 0.4180(0.5239) Encoder Grad: 0.1441  Decoder Grad: 0.2025  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46400/57574] Data 0.000 (0.000) Elapsed 559m 48s (remain 134m 47s) Loss: 0.4962(0.5238) Encoder Grad: 0.2225  Decoder Grad: 0.2653  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46500/57574] Data 0.000 (0.000) Elapsed 560m 58s (remain 133m 34s) Loss: 0.4257(0.5237) Encoder Grad: 0.0446  Decoder Grad: 0.1681  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46600/57574] Data 0.000 (0.000) Elapsed 562m 8s (remain 132m 21s) Loss: 0.4949(0.5235) Encoder Grad: 0.0793  Decoder Grad: 0.2070  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46700/57574] Data 0.000 (0.000) Elapsed 563m 17s (remain 131m 8s) Loss: 0.4339(0.5234) Encoder Grad: 0.1847  Decoder Grad: 0.2567  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46800/57574] Data 0.000 (0.000) Elapsed 564m 27s (remain 129m 55s) Loss: 0.4543(0.5232) Encoder Grad: 0.3471  Decoder Grad: 0.2442  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][46900/57574] Data 0.000 (0.000) Elapsed 565m 37s (remain 128m 43s) Loss: 0.3765(0.5231) Encoder Grad: 0.0615  Decoder Grad: 0.2456  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47000/57574] Data 0.000 (0.000) Elapsed 566m 47s (remain 127m 30s) Loss: 0.5075(0.5229) Encoder Grad: 0.0770  Decoder Grad: 0.2021  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47100/57574] Data 0.000 (0.000) Elapsed 567m 57s (remain 126m 17s) Loss: 0.4402(0.5228) Encoder Grad: 0.1093  Decoder Grad: 0.2337  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47200/57574] Data 0.000 (0.000) Elapsed 569m 7s (remain 125m 4s) Loss: 0.4798(0.5226) Encoder Grad: 0.1179  Decoder Grad: 0.2091  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47300/57574] Data 0.000 (0.000) Elapsed 570m 17s (remain 123m 51s) Loss: 0.4737(0.5225) Encoder Grad: 0.1595  Decoder Grad: 0.2471  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47400/57574] Data 0.000 (0.000) Elapsed 571m 27s (remain 122m 38s) Loss: 0.4431(0.5224) Encoder Grad: 0.1388  Decoder Grad: 0.2533  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47500/57574] Data 0.000 (0.000) Elapsed 572m 36s (remain 121m 25s) Loss: 0.5262(0.5222) Encoder Grad: 0.1351  Decoder Grad: 0.3469  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47600/57574] Data 0.000 (0.000) Elapsed 573m 46s (remain 120m 12s) Loss: 0.4772(0.5221) Encoder Grad: 0.3830  Decoder Grad: 0.2878  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47700/57574] Data 0.000 (0.000) Elapsed 574m 57s (remain 119m 0s) Loss: 0.4773(0.5220) Encoder Grad: 0.2074  Decoder Grad: 0.2383  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47800/57574] Data 0.000 (0.000) Elapsed 576m 8s (remain 117m 47s) Loss: 0.4116(0.5218) Encoder Grad: 0.0830  Decoder Grad: 0.2190  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][47900/57574] Data 0.000 (0.000) Elapsed 577m 19s (remain 116m 34s) Loss: 0.4655(0.5217) Encoder Grad: 0.4699  Decoder Grad: 0.2511  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][48000/57574] Data 0.000 (0.000) Elapsed 578m 28s (remain 115m 21s) Loss: 0.4448(0.5216) Encoder Grad: 0.2039  Decoder Grad: 0.2209  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][48100/57574] Data 0.000 (0.000) Elapsed 579m 36s (remain 114m 8s) Loss: 0.4350(0.5214) Encoder Grad: 0.1934  Decoder Grad: 0.1993  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][48200/57574] Data 0.000 (0.000) Elapsed 580m 44s (remain 112m 55s) Loss: 0.4288(0.5213) Encoder Grad: 0.9129  Decoder Grad: 0.2824  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][48300/57574] Data 0.000 (0.000) Elapsed 581m 53s (remain 111m 42s) Loss: 0.3659(0.5211) Encoder Grad: 0.2963  Decoder Grad: 0.2300  Encoder LR: 0.000097  Decoder LR: 0.000400  \n",
      "Epoch: [1][48400/57574] Data 0.000 (0.000) Elapsed 583m 0s (remain 110m 29s) Loss: 0.4568(0.5210) Encoder Grad: 0.0299  Decoder Grad: 0.1775  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][48500/57574] Data 0.000 (0.000) Elapsed 584m 8s (remain 109m 16s) Loss: 0.4115(0.5209) Encoder Grad: 0.1136  Decoder Grad: 0.2996  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][48600/57574] Data 0.000 (0.000) Elapsed 585m 16s (remain 108m 3s) Loss: 0.4201(0.5207) Encoder Grad: 0.1277  Decoder Grad: 0.2324  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][48700/57574] Data 0.000 (0.000) Elapsed 586m 24s (remain 106m 50s) Loss: 0.4584(0.5206) Encoder Grad: 0.5206  Decoder Grad: 0.3751  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][48800/57574] Data 0.000 (0.000) Elapsed 587m 31s (remain 105m 37s) Loss: 0.4105(0.5205) Encoder Grad: 0.0473  Decoder Grad: 0.2064  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][48900/57574] Data 0.000 (0.000) Elapsed 588m 39s (remain 104m 24s) Loss: 0.4391(0.5203) Encoder Grad: 0.0902  Decoder Grad: 0.1999  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49000/57574] Data 0.000 (0.000) Elapsed 589m 46s (remain 103m 11s) Loss: 0.4688(0.5202) Encoder Grad: 0.1293  Decoder Grad: 0.3024  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49100/57574] Data 0.000 (0.000) Elapsed 590m 54s (remain 101m 58s) Loss: 0.4641(0.5201) Encoder Grad: 0.0386  Decoder Grad: 0.2022  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49200/57574] Data 0.000 (0.000) Elapsed 592m 2s (remain 100m 45s) Loss: 0.5133(0.5200) Encoder Grad: 0.3920  Decoder Grad: 0.4210  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49300/57574] Data 0.000 (0.000) Elapsed 593m 10s (remain 99m 32s) Loss: 0.4387(0.5200) Encoder Grad: 0.1070  Decoder Grad: 0.2436  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49400/57574] Data 0.000 (0.000) Elapsed 594m 18s (remain 98m 19s) Loss: 0.4589(0.5199) Encoder Grad: 0.6747  Decoder Grad: 0.3694  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49500/57574] Data 0.000 (0.000) Elapsed 595m 26s (remain 97m 6s) Loss: 0.4414(0.5198) Encoder Grad: 0.0851  Decoder Grad: 0.2508  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49600/57574] Data 0.000 (0.000) Elapsed 596m 34s (remain 95m 53s) Loss: 0.5217(0.5197) Encoder Grad: 0.4105  Decoder Grad: 0.4218  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49700/57574] Data 0.000 (0.000) Elapsed 597m 41s (remain 94m 40s) Loss: 0.5099(0.5195) Encoder Grad: 0.0354  Decoder Grad: 0.2072  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49800/57574] Data 0.000 (0.000) Elapsed 598m 49s (remain 93m 27s) Loss: 0.4969(0.5194) Encoder Grad: 0.2091  Decoder Grad: 0.2945  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][49900/57574] Data 0.000 (0.000) Elapsed 599m 57s (remain 92m 15s) Loss: 0.4736(0.5193) Encoder Grad: 0.0601  Decoder Grad: 0.2089  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50000/57574] Data 0.000 (0.000) Elapsed 601m 5s (remain 91m 2s) Loss: 0.3947(0.5192) Encoder Grad: 0.1774  Decoder Grad: 0.2450  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50100/57574] Data 0.000 (0.000) Elapsed 602m 13s (remain 89m 49s) Loss: 0.4160(0.5190) Encoder Grad: 0.0446  Decoder Grad: 0.1940  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50200/57574] Data 0.000 (0.000) Elapsed 603m 22s (remain 88m 36s) Loss: 0.4954(0.5189) Encoder Grad: 0.0732  Decoder Grad: 0.1988  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50300/57574] Data 0.000 (0.000) Elapsed 604m 30s (remain 87m 24s) Loss: 0.4888(0.5188) Encoder Grad: 0.1686  Decoder Grad: 0.2879  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50400/57574] Data 0.000 (0.000) Elapsed 605m 38s (remain 86m 11s) Loss: 0.4560(0.5187) Encoder Grad: 0.4132  Decoder Grad: 0.6330  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50500/57574] Data 0.000 (0.000) Elapsed 606m 45s (remain 84m 58s) Loss: 0.3859(0.5186) Encoder Grad: 0.0588  Decoder Grad: 0.2144  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50600/57574] Data 0.000 (0.000) Elapsed 607m 53s (remain 83m 46s) Loss: 0.4266(0.5184) Encoder Grad: 0.2968  Decoder Grad: 0.2504  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50700/57574] Data 0.000 (0.000) Elapsed 609m 1s (remain 82m 33s) Loss: 0.4794(0.5183) Encoder Grad: 0.0402  Decoder Grad: 0.2051  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50800/57574] Data 0.000 (0.000) Elapsed 610m 9s (remain 81m 20s) Loss: 0.4291(0.5182) Encoder Grad: 0.0405  Decoder Grad: 0.2148  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][50900/57574] Data 0.000 (0.000) Elapsed 611m 17s (remain 80m 8s) Loss: 0.4667(0.5180) Encoder Grad: 0.0496  Decoder Grad: 0.2243  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51000/57574] Data 0.000 (0.000) Elapsed 612m 25s (remain 78m 55s) Loss: 0.4153(0.5179) Encoder Grad: 0.1843  Decoder Grad: 0.2235  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51100/57574] Data 0.000 (0.000) Elapsed 613m 34s (remain 77m 43s) Loss: 0.5013(0.5177) Encoder Grad: 0.1062  Decoder Grad: 0.2176  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51200/57574] Data 0.000 (0.000) Elapsed 614m 42s (remain 76m 30s) Loss: 0.4527(0.5176) Encoder Grad: 0.0982  Decoder Grad: 0.2356  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51300/57574] Data 0.000 (0.000) Elapsed 615m 50s (remain 75m 18s) Loss: 0.4140(0.5175) Encoder Grad: 0.0871  Decoder Grad: 0.2372  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51400/57574] Data 0.000 (0.000) Elapsed 616m 58s (remain 74m 5s) Loss: 0.4637(0.5174) Encoder Grad: 0.2148  Decoder Grad: 0.2366  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51500/57574] Data 0.000 (0.000) Elapsed 618m 6s (remain 72m 53s) Loss: 0.5041(0.5173) Encoder Grad: 0.1640  Decoder Grad: 0.2178  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51600/57574] Data 0.000 (0.000) Elapsed 619m 14s (remain 71m 40s) Loss: 0.4546(0.5171) Encoder Grad: 0.2923  Decoder Grad: 0.3235  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51700/57574] Data 0.000 (0.000) Elapsed 620m 23s (remain 70m 28s) Loss: 0.4665(0.5170) Encoder Grad: 0.2877  Decoder Grad: 0.2919  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51800/57574] Data 0.000 (0.000) Elapsed 621m 31s (remain 69m 15s) Loss: 0.4074(0.5169) Encoder Grad: 0.1979  Decoder Grad: 0.2892  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][51900/57574] Data 0.000 (0.000) Elapsed 622m 39s (remain 68m 3s) Loss: 0.4760(0.5168) Encoder Grad: 0.0686  Decoder Grad: 0.2029  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52000/57574] Data 0.000 (0.000) Elapsed 623m 47s (remain 66m 51s) Loss: 0.4618(0.5167) Encoder Grad: 0.0425  Decoder Grad: 0.1873  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52100/57574] Data 0.000 (0.000) Elapsed 624m 55s (remain 65m 38s) Loss: 0.4646(0.5166) Encoder Grad: 0.6329  Decoder Grad: 0.5275  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52200/57574] Data 0.000 (0.000) Elapsed 626m 3s (remain 64m 26s) Loss: 0.4422(0.5165) Encoder Grad: 0.0604  Decoder Grad: 0.2476  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52300/57574] Data 0.000 (0.000) Elapsed 627m 11s (remain 63m 13s) Loss: 0.4388(0.5164) Encoder Grad: 0.0560  Decoder Grad: 0.2344  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52400/57574] Data 0.000 (0.000) Elapsed 628m 19s (remain 62m 1s) Loss: 0.4472(0.5162) Encoder Grad: 0.0295  Decoder Grad: 0.2448  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52500/57574] Data 0.000 (0.000) Elapsed 629m 27s (remain 60m 49s) Loss: 0.4529(0.5161) Encoder Grad: 0.1660  Decoder Grad: 0.2158  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52600/57574] Data 0.000 (0.000) Elapsed 630m 34s (remain 59m 36s) Loss: 0.3936(0.5160) Encoder Grad: 0.2084  Decoder Grad: 0.2855  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52700/57574] Data 0.000 (0.000) Elapsed 631m 42s (remain 58m 24s) Loss: 0.5156(0.5159) Encoder Grad: 0.1008  Decoder Grad: 0.2305  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52800/57574] Data 0.000 (0.000) Elapsed 632m 50s (remain 57m 12s) Loss: 0.4400(0.5158) Encoder Grad: 0.0323  Decoder Grad: 0.2277  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][52900/57574] Data 0.000 (0.000) Elapsed 633m 58s (remain 56m 0s) Loss: 0.5193(0.5156) Encoder Grad: 0.0793  Decoder Grad: 0.2461  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53000/57574] Data 0.000 (0.000) Elapsed 635m 6s (remain 54m 47s) Loss: 0.4429(0.5155) Encoder Grad: 0.0521  Decoder Grad: 0.2267  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53100/57574] Data 0.000 (0.000) Elapsed 636m 13s (remain 53m 35s) Loss: 0.4189(0.5154) Encoder Grad: 0.1680  Decoder Grad: 0.2707  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53200/57574] Data 0.000 (0.000) Elapsed 637m 22s (remain 52m 23s) Loss: 0.4484(0.5153) Encoder Grad: 0.0516  Decoder Grad: 0.2277  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53300/57574] Data 0.000 (0.000) Elapsed 638m 29s (remain 51m 11s) Loss: 0.4430(0.5151) Encoder Grad: 0.1894  Decoder Grad: 0.2186  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53400/57574] Data 0.000 (0.000) Elapsed 639m 37s (remain 49m 59s) Loss: 0.3857(0.5150) Encoder Grad: 0.0632  Decoder Grad: 0.2487  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53500/57574] Data 0.000 (0.000) Elapsed 640m 46s (remain 48m 46s) Loss: 0.4580(0.5149) Encoder Grad: 0.0602  Decoder Grad: 0.2469  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53600/57574] Data 0.000 (0.000) Elapsed 641m 53s (remain 47m 34s) Loss: 0.4857(0.5148) Encoder Grad: 0.0319  Decoder Grad: 0.2076  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53700/57574] Data 0.000 (0.000) Elapsed 643m 2s (remain 46m 22s) Loss: 0.4646(0.5147) Encoder Grad: 0.1268  Decoder Grad: 0.2502  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53800/57574] Data 0.000 (0.000) Elapsed 644m 10s (remain 45m 10s) Loss: 0.4783(0.5146) Encoder Grad: 0.0918  Decoder Grad: 0.2548  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][53900/57574] Data 0.000 (0.000) Elapsed 645m 18s (remain 43m 58s) Loss: 0.4335(0.5145) Encoder Grad: 0.1143  Decoder Grad: 0.1837  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54000/57574] Data 0.000 (0.000) Elapsed 646m 26s (remain 42m 46s) Loss: 0.4481(0.5144) Encoder Grad: 0.0413  Decoder Grad: 0.2661  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54100/57574] Data 0.000 (0.000) Elapsed 647m 34s (remain 41m 34s) Loss: 0.4021(0.5142) Encoder Grad: 0.2191  Decoder Grad: 0.2251  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54200/57574] Data 0.000 (0.000) Elapsed 648m 41s (remain 40m 22s) Loss: 0.4740(0.5141) Encoder Grad: 0.0597  Decoder Grad: 0.2162  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54300/57574] Data 0.000 (0.000) Elapsed 649m 49s (remain 39m 10s) Loss: 0.4386(0.5140) Encoder Grad: 0.1121  Decoder Grad: 0.2396  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54400/57574] Data 0.000 (0.000) Elapsed 650m 58s (remain 37m 58s) Loss: 0.4125(0.5139) Encoder Grad: 0.1291  Decoder Grad: 0.2601  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54500/57574] Data 0.000 (0.000) Elapsed 652m 5s (remain 36m 46s) Loss: 0.4031(0.5137) Encoder Grad: 0.0318  Decoder Grad: 0.2196  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54600/57574] Data 0.000 (0.000) Elapsed 653m 14s (remain 35m 34s) Loss: 0.4709(0.5136) Encoder Grad: 0.0552  Decoder Grad: 0.1922  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54700/57574] Data 0.000 (0.000) Elapsed 654m 22s (remain 34m 22s) Loss: 0.4510(0.5135) Encoder Grad: 0.0689  Decoder Grad: 0.2106  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54800/57574] Data 0.000 (0.000) Elapsed 655m 30s (remain 33m 10s) Loss: 0.4932(0.5134) Encoder Grad: 0.0457  Decoder Grad: 0.2353  Encoder LR: 0.000096  Decoder LR: 0.000400  \n",
      "Epoch: [1][54900/57574] Data 0.000 (0.000) Elapsed 656m 38s (remain 31m 58s) Loss: 0.3686(0.5133) Encoder Grad: 0.0497  Decoder Grad: 0.2106  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55000/57574] Data 0.000 (0.000) Elapsed 657m 46s (remain 30m 46s) Loss: 0.4760(0.5131) Encoder Grad: 0.2337  Decoder Grad: 0.3397  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55100/57574] Data 0.000 (0.000) Elapsed 658m 54s (remain 29m 34s) Loss: 0.4459(0.5130) Encoder Grad: 0.2557  Decoder Grad: 0.3423  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55200/57574] Data 0.000 (0.000) Elapsed 660m 2s (remain 28m 22s) Loss: 0.3898(0.5129) Encoder Grad: 0.1349  Decoder Grad: 0.2290  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55300/57574] Data 0.000 (0.000) Elapsed 661m 10s (remain 27m 10s) Loss: 0.4502(0.5128) Encoder Grad: 0.0290  Decoder Grad: 0.2336  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55400/57574] Data 0.000 (0.000) Elapsed 662m 18s (remain 25m 58s) Loss: 0.4312(0.5127) Encoder Grad: 0.0556  Decoder Grad: 0.1965  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55500/57574] Data 0.000 (0.000) Elapsed 663m 26s (remain 24m 46s) Loss: 0.3839(0.5125) Encoder Grad: 0.2282  Decoder Grad: 0.2092  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55600/57574] Data 0.000 (0.000) Elapsed 664m 34s (remain 23m 34s) Loss: 0.4326(0.5124) Encoder Grad: 0.1688  Decoder Grad: 0.3028  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55700/57574] Data 0.000 (0.000) Elapsed 665m 42s (remain 22m 23s) Loss: 0.4891(0.5123) Encoder Grad: 0.1011  Decoder Grad: 0.2319  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55800/57574] Data 0.000 (0.000) Elapsed 666m 50s (remain 21m 11s) Loss: 0.4678(0.5122) Encoder Grad: 0.0338  Decoder Grad: 0.2049  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][55900/57574] Data 0.000 (0.000) Elapsed 667m 57s (remain 19m 59s) Loss: 0.4781(0.5121) Encoder Grad: 0.0441  Decoder Grad: 0.1999  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56000/57574] Data 0.000 (0.000) Elapsed 669m 5s (remain 18m 47s) Loss: 0.4436(0.5119) Encoder Grad: 0.1402  Decoder Grad: 0.2294  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56100/57574] Data 0.000 (0.000) Elapsed 670m 13s (remain 17m 35s) Loss: 0.4196(0.5118) Encoder Grad: 0.3238  Decoder Grad: 0.3155  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56200/57574] Data 0.000 (0.000) Elapsed 671m 21s (remain 16m 24s) Loss: 0.4946(0.5117) Encoder Grad: 0.2480  Decoder Grad: 0.6636  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56300/57574] Data 0.000 (0.000) Elapsed 672m 29s (remain 15m 12s) Loss: 0.4566(0.5116) Encoder Grad: 0.1026  Decoder Grad: 0.2353  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56400/57574] Data 0.000 (0.000) Elapsed 673m 37s (remain 14m 0s) Loss: 0.5057(0.5115) Encoder Grad: 0.0748  Decoder Grad: 0.2317  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56500/57574] Data 0.000 (0.000) Elapsed 674m 44s (remain 12m 48s) Loss: 0.4269(0.5114) Encoder Grad: 0.1890  Decoder Grad: 0.3471  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56600/57574] Data 0.000 (0.000) Elapsed 675m 53s (remain 11m 37s) Loss: 0.4819(0.5113) Encoder Grad: 0.0559  Decoder Grad: 0.2446  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56700/57574] Data 0.000 (0.000) Elapsed 677m 0s (remain 10m 25s) Loss: 0.4649(0.5112) Encoder Grad: 0.0549  Decoder Grad: 0.2819  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56800/57574] Data 0.000 (0.000) Elapsed 678m 8s (remain 9m 13s) Loss: 0.4223(0.5110) Encoder Grad: 0.1934  Decoder Grad: 0.2700  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][56900/57574] Data 0.000 (0.000) Elapsed 679m 17s (remain 8m 2s) Loss: 0.4703(0.5109) Encoder Grad: 0.3134  Decoder Grad: 0.5993  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57000/57574] Data 0.000 (0.000) Elapsed 680m 25s (remain 6m 50s) Loss: 0.4370(0.5108) Encoder Grad: 0.0475  Decoder Grad: 0.1980  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57100/57574] Data 0.000 (0.000) Elapsed 681m 33s (remain 5m 38s) Loss: 0.4651(0.5107) Encoder Grad: 0.3397  Decoder Grad: 0.2918  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57200/57574] Data 0.000 (0.000) Elapsed 682m 41s (remain 4m 27s) Loss: 0.5292(0.5106) Encoder Grad: 0.0460  Decoder Grad: 0.2179  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57300/57574] Data 0.000 (0.000) Elapsed 683m 49s (remain 3m 15s) Loss: 0.4811(0.5105) Encoder Grad: 0.0306  Decoder Grad: 0.1906  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57400/57574] Data 0.000 (0.000) Elapsed 684m 57s (remain 2m 3s) Loss: 0.4488(0.5104) Encoder Grad: 0.0867  Decoder Grad: 0.2181  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57500/57574] Data 0.000 (0.000) Elapsed 686m 5s (remain 0m 52s) Loss: 0.5054(0.5103) Encoder Grad: 0.1182  Decoder Grad: 0.2281  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "Epoch: [1][57573/57574] Data 0.000 (0.000) Elapsed 686m 55s (remain 0m 0s) Loss: 0.4792(0.5103) Encoder Grad: 0.0494  Decoder Grad: 0.2248  Encoder LR: 0.000095  Decoder LR: 0.000400  \n",
      "EVAL: [0/3031] Data 1.651 (1.651) Elapsed 0m 1s (remain 96m 54s) \n",
      "EVAL: [100/3031] Data 0.000 (0.017) Elapsed 0m 19s (remain 9m 35s) \n",
      "EVAL: [200/3031] Data 0.000 (0.008) Elapsed 0m 38s (remain 8m 57s) \n",
      "EVAL: [300/3031] Data 0.000 (0.006) Elapsed 0m 56s (remain 8m 31s) \n",
      "EVAL: [400/3031] Data 0.000 (0.004) Elapsed 1m 14s (remain 8m 10s) \n",
      "EVAL: [500/3031] Data 0.000 (0.003) Elapsed 1m 32s (remain 7m 49s) \n",
      "EVAL: [600/3031] Data 0.000 (0.003) Elapsed 1m 51s (remain 7m 28s) \n",
      "EVAL: [700/3031] Data 0.000 (0.003) Elapsed 2m 9s (remain 7m 9s) \n",
      "EVAL: [800/3031] Data 0.000 (0.002) Elapsed 2m 27s (remain 6m 50s) \n",
      "EVAL: [900/3031] Data 0.000 (0.002) Elapsed 2m 45s (remain 6m 30s) \n",
      "EVAL: [1000/3031] Data 0.000 (0.002) Elapsed 3m 4s (remain 6m 13s) \n",
      "EVAL: [1100/3031] Data 0.000 (0.002) Elapsed 3m 23s (remain 5m 56s) \n",
      "EVAL: [1200/3031] Data 0.000 (0.002) Elapsed 3m 42s (remain 5m 39s) \n",
      "EVAL: [1300/3031] Data 0.000 (0.001) Elapsed 4m 1s (remain 5m 21s) \n",
      "EVAL: [1400/3031] Data 0.000 (0.001) Elapsed 4m 20s (remain 5m 3s) \n",
      "EVAL: [1500/3031] Data 0.000 (0.001) Elapsed 4m 39s (remain 4m 45s) \n",
      "EVAL: [1600/3031] Data 0.000 (0.001) Elapsed 4m 58s (remain 4m 26s) \n",
      "EVAL: [1700/3031] Data 0.000 (0.001) Elapsed 5m 17s (remain 4m 7s) \n",
      "EVAL: [1800/3031] Data 0.000 (0.001) Elapsed 5m 36s (remain 3m 49s) \n",
      "EVAL: [1900/3031] Data 0.000 (0.001) Elapsed 5m 54s (remain 3m 31s) \n",
      "EVAL: [2000/3031] Data 0.000 (0.001) Elapsed 6m 13s (remain 3m 12s) \n",
      "EVAL: [2100/3031] Data 0.000 (0.001) Elapsed 6m 32s (remain 2m 53s) \n",
      "EVAL: [2200/3031] Data 0.000 (0.001) Elapsed 6m 50s (remain 2m 34s) \n",
      "EVAL: [2300/3031] Data 0.000 (0.001) Elapsed 7m 9s (remain 2m 16s) \n",
      "EVAL: [2400/3031] Data 0.000 (0.001) Elapsed 7m 29s (remain 1m 57s) \n",
      "EVAL: [2500/3031] Data 0.000 (0.001) Elapsed 7m 50s (remain 1m 39s) \n",
      "EVAL: [2600/3031] Data 0.000 (0.001) Elapsed 8m 13s (remain 1m 21s) \n",
      "EVAL: [2700/3031] Data 0.000 (0.001) Elapsed 8m 35s (remain 1m 2s) \n",
      "EVAL: [2800/3031] Data 0.000 (0.001) Elapsed 8m 56s (remain 0m 44s) \n",
      "EVAL: [2900/3031] Data 0.000 (0.001) Elapsed 9m 19s (remain 0m 25s) \n",
      "EVAL: [3000/3031] Data 0.000 (0.001) Elapsed 9m 42s (remain 0m 5s) \n",
      "EVAL: [3030/3031] Data 0.000 (0.001) Elapsed 9m 48s (remain 0m 0s) \n",
      "labels: ['InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-24-14)11-15(21)19-17(16(22)23)8-4-2-3-5-9-17/h6-7,10,13H,2-5,8-9,11H2,1H3,(H,18,20)(H,19,21)(H,22,23)'\n",
      " 'InChI=1S/C10H15N5S/c1-7-6-8(9(11)12)14-10(13-7)15-2-4-16-5-3-15/h6H,2-5H2,1H3,(H3,11,12)'\n",
      " 'InChI=1S/C16H16Cl2N2O3/c17-12-3-1-11(2-4-12)13-14(18)16(22)20(15(13)21)6-5-19-7-9-23-10-8-19/h1-4H,5-10H2'\n",
      " 'InChI=1S/C17H14N4/c1-2-9-21-12-14(20-17(21)6-1)11-19-16-5-3-4-13-10-18-8-7-15(13)16/h1-10,12,19H,11H2'\n",
      " 'InChI=1S/C15H18O3/c1-13(2)9-14(3)15(4,18-12(16)17-14)11-8-6-5-7-10(11)13/h5-8H,9H2,1-4H3/t14-,15+/m1/s1']\n",
      "preds: ['InChI=1S/C16H22N2O4S/c1-10(20)17-14(11-6-7-12)15(21)18-13(8-9-14(19)20)16(23)4-2-3-5-16/h6-7,11,20H,2-5,8-9H2,1H3,(H,17,21)(H,18,22)(H,21,22)', 'InChI=1S/C10H15N5S/c1-7-6-8(11)9(14-10(7)15)15-4-2-12-3-5-15/h6H,2-5H2,1H3,(H2,11,14)(H,12,15)', 'InChI=1S/C16H15Cl2N3O3/c17-11-3-4-12(13(18)9-11)14(22)20-5-7-21(8-6-20)15(23)10-1-2-18-17(10)24/h1-4,9,22H,5-8H2,(H,19,23)', 'InChI=1S/C17H14N4/c1-2-6-15-14(4-1)19-16(20-15)11-20-15-7-3-5-12-8-9-19-10-13(12)15/h1-10,19H,11H2,(H,20,21)', 'InChI=1S/C15H18O4/c1-10-9-13(17)19-14(2,3)8-12(16)16(10)11-6-4-5-7-13(11)17/h4-7,12H,8-9H2,1-3H3/t12-,15+/m1/s1']\n",
      "Epoch 1 - avg_train_loss: 0.5103  time: 41807s\n",
      "Epoch 1 - Score: 56.3670\n",
      "Epoch 1 - Save Best Score: 56.3670 Model\n",
      "Epoch: [2][0/57574] Data 2.507 (2.507) Elapsed 0m 3s (remain 3216m 38s) Loss: 0.5110(0.5110) Encoder Grad: 0.1132  Decoder Grad: 0.3111  Encoder LR: 0.000095  Decoder LR: 0.000400  \n"
     ]
    }
   ],
   "source": [
    "train_loop(folds, CFG.trn_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0ce8efb1f28dd4a0568a606b813efaa2aeaaae67617ecb5fd40574a9cea5029cb",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ce8efb1f28dd4a0568a606b813efaa2aeaaae67617ecb5fd40574a9cea5029cb"
   }
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2420.3766,
   "end_time": "2021-04-26T13:34:09.349906",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-26T12:53:48.973306",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}